[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Quantum of Science",
    "section": "",
    "text": "Peer-Reviewed Publications\n\n\nThis page lists peer-reviewed journal articles. Author order and formatting follow the published versions.\n\n\n\nJournal Articles\n\n\n\n Anže Zupanc, Joseph Install, Timo Weckman, Marko M. Melander, Marianna Kemell, Karoliina Honkala, Timo Juhani Repo ,  “Sustainable urban mining of silver with fatty acids” , Chemical Engineering Journal 512, 162129 (2025).\n\n\n Timo Weckman ,  “Dispersion with Fixed Diagonal Matrices: Exchange energy correction and an assessment of the Becke–Roussel exchange hole” , Journal of Chemical Physics 162(5), 054112 (2025).\n\n\n Sherif Hegazy, Hanan H. Ibrahim, Timo Weckman, Tao Hu, Sari Tuomikoski, Ulla Lassi, Karoliina Honkala, Varsha Srivastava ,  “Synergistic pyrolysis of Cellulose/Fe-MOF composite: A combined experimental and DFT study on dye removal” , Chemical Engineering Journal 504, 158654 (2024).\n\n\n Jayanta Dana, M. R. Ajayakumar, Alexander Efimov, Timo Weckman, Karoliina Honkala, Nikolai V. Tkachenko ,  “Structure-dependent activation of a Co molecular catalyst through photoinduced electron transfer from a CdTe quantum dot” , Nanoscale 16(44), 20725–20737 (2024).\n\n\n Mario Mäkinen, Timo Weckman, Kari Laasonen ,  “Modelling the growth reaction pathways of zincone ALD/MLD hybrid thin films: A DFT study” , Physical Chemistry Chemical Physics 26(24), 17334–17344 (2024).\n\n\n Hanan Ibrahim, Timo Weckman, Dmitry Murzin, Karoliina Honkala ,  “Understanding selective hydrogenation of phenylacetylene on PdAg single-atom alloys: DFT insights on molecule size and surface ensemble effects” , Journal of Catalysis 434, 115523 (2024).\n\n\n Anže Zupanc, Joseph Install, Timo Weckman, Marko M. Melander, Mikko J. Heikkilä, Marianna Kemell, Karoliina Honkala, Timo Juhani Repo ,  “Sequential selective dissolution of coinage metals in recyclable ionic media” , Angewandte Chemie 136(31), e202407147 (2024).\n\n\n Jayanta Dana, Ramsha Khan, Timo Weckman, Karoliina Honkala, Nikolai V. Tkachenko ,  “Laterally bound Co porphyrin on CdTe quantum dots: A long-lived charge-separated nanocomposite” , Journal of Physical Chemistry C 127(21), 10164–10173 (2024).\n\n\n Marko M. Melander, Tongwei Wu, Timo Weckman, Karoliina Honkala ,  “Constant inner potential DFT for modelling electrochemical systems under constant potential and bias” , npj Computational Materials 10(1), 5 (2023).\n\n\n Luca Mastroianni, Timo Weckman, Kari Eränen, Vincenzo Russo, Dmitry Yu. Murzin, Karoliina Honkala, Tapio Salmi ,  “Oxidative dehydrogenation of alcohols on gold: An experimental and computational study on the role of water and alcohol chain length” , Journal of Catalysis 425, 233–244 (2023).\n\n\n Derk Kooi, Timo Weckman, Paola Gori-Giorgi ,  “Dispersion without many-body density distortion: Assessment on atoms and small molecules” , Journal of Chemical Theory and Computation 17(4), 2283–2293 (2021).\n\n\n Timo Weckman, Mahdi Shirazi, Simon D. Elliott, Kari Laasonen ,  “Kinetic Monte Carlo study of the atomic layer deposition of zinc oxide” , Journal of Physical Chemistry C 122(47), 27044–27058 (2018).\n\n\n Timo Weckman, Kari Laasonen ,  “Atomic layer deposition of zinc oxide: Study on the water pulse reactions from first principles” , Journal of Physical Chemistry C 122(14), 7685–7694 (2018).\n\n\n Li Ma, Marko M. Melander, Timo Weckman, Kari Laasonen, Jaakko Ahola ,  “CO oxidation on the Au15Cu15 cluster and the role of vacancies in the MgO(100) support” , Journal of Physical Chemistry C 120(47), 26747–26758 (2016).\n\n\n Timo Weckman, Kari Laasonen ,  “Atomic layer deposition of zinc oxide: Diethyl zinc reactions and surface saturation from first principles” , Journal of Physical Chemistry C 120(38), 21460–21471 (2016).\n\n\n Li Ma, Marko M. Melander, Timo Weckman, Saana Lipasti, Jaakko Ahola, Kari Laasonen ,  “DFT simulations and microkinetic modelling of 1-pentyne hydrogenation on Cu20 model catalysts” , Journal of Molecular Graphics and Modelling 65, 61–170 (2016).\n\n\n Timo Weckman, Kari Laasonen ,  “First-principles study of the atomic layer deposition of alumina by the TMA–H2O process” , Physical Chemistry Chemical Physics 17(26), 17322–17334 (2015).\n\n\n Maryam Borghei, Gianmario Scotti, Petri Kanninen, Timo Weckman, Ilya V. Anoshkin, Albert G. Nasibulin, Sami Franssila, Esko I. Kauppinen, Tanja Kallio, Virginia Ruiz ,  “Enhanced performance of a silicon microfabricated direct methanol fuel cell with PtRu catalysts supported on few-walled carbon nanotubes” , Energy 65, 612–620 (2014).\n\n&lt;/ol&gt;\n\n\nA complete list, including supplementary material and data, is available on Google Scholar.\n\n\n\n\n \n\n    © 2026 Timo Weckman"
  },
  {
    "objectID": "blog_tddft_c6.html",
    "href": "blog_tddft_c6.html",
    "title": "Dispersion coefficients from TDDFT (example with PySCF)",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nDispersion interaction is a weak attractive interaction that binds even neutral, noble gas atoms to each other. Dispersion interaction sometimes mistakenly called the van der Waals (vdW) interaction, which is not correct. VdW-interactions are a combination of intermolecular interactions, where permanent molecular dipoles induce and interact with other molecular dipoles. Despite being the weakest form of van der Waals forces, dispersion interactions significantly influence the structure, stability, and properties of molecular systems, including biological molecules like proteins and DNA.\nDispersion interaction is caused by fluctuations in the electron densities in an atom or a molecule. The fluctuations cause intantaneous electric fields that are felt by ther nearby atoms or molecules, which respond to the electric field by adjusting their own electron densities accordingly. These instantaneous dipole moments then attract each other, resulting in the dispersion interaction.\nWhile other van der Waals force are well described by the computational chemistry workhorse, the density functional theory (DFT), the dispersion interaction is not. Traditional DFT methods struggle to accurately predict dispersion interaction, because the electron-electron interaction is only treated approximately, without directly accounting for the correlated motion of electrons. Instead, the dispersion is usually included in the DFT calculations as a separate correction. Using the Rayleigh–Schrödinger perturbation theory, one can show that the potential energy between two weakly interacting systems is proportional to the inverse sixth power of the distance between them, leading to the well-known\n\\[V(r) \\propto -\\frac{C_6}{r^6}\\]\nwhere \\(C_6\\) is the dispersion coefficient and \\(r\\) is the intermolecular distance.\nHere, we will have a look at how the dispersion coefficient \\(C_6\\) can be computed using time-dependent density functional theory (TD-DFT) by PySCF. We will also explore the basis set dependence of the results and how the commonly employed Tamm-Dancoff approximation (TDA) affects the results.\n\nOscillator strengths\nOscillator strength describes the probability for transition between quantum states when a photon is absorbed (or emitted),\n\\[f_n = \\frac{2}{3} \\omega_{n0}^A\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\]\nwhere \\(f_n\\) is the oscillator strength for a transition from the ground state to the \\(n\\)th excited state, \\(\\omega_n\\) is the corresponding transition energy and \\(\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\) is the squared magnitude for the transition dipole moment vector of the excitation.\nIn fact, one can use Rayleigh–Schrödinger perturbation theory to derive an expression for the dispersion interaction between two systems \\(A\\) and \\(B\\) separated by a large distance \\(R\\) with no overlap:\n\\[\nU_\\text{disp} = -\\frac{C_6}{R^6}\n\\]\n\\[\nC^{AB}_6 = \\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\left|\\left&lt;{\\phi_0^B|\\mathbf{r}|\\phi_m^B}\\right&gt;\\right|^2}{\\omega_{n0}^A + \\omega_{m0}^B} = \\frac{3}{2}\\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{f_{n0}^A f_{m0}^B}{\\omega_{n0}^A\\omega_{m0}^B(\\omega_{n0}^A + \\omega_{m0}^B)},\n\\]\nwhere \\(\\omega_{m0} = E_m-E_0\\).\nHere we will use the last sum to compute the dispersion coefficient \\(C_6\\) for a methane molecule.\n\n\nPySCF\nWe will use PySCF to compute the excitation spectra and oscillator strenghts using time-dependent density functional theory (TD-DFT). First we’ll compute the ground state energy of a methane molecule using \\(\\omega\\)B97X exchange–correlation functional and a diffuse triple-\\(\\zeta\\) basis set def2-TZVPD.\n\nfrom pyscf import gto, scf, tdscf, dft, tddft\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define molecule and basis set\nmol = gto.Mole()\nmol.atom = 'H 0.5349075 0.1628792 0.9466881; \\\n            H 0.2074921 0.8335188 -0.6864069; \\\n            H 0.3383696 -0.9421607 -0.4547893; \\\n            H -1.0808570 -0.0543245 0.1943354; \\\n            C -0.0000122 -0.0000128 -0.0000272'\nmol.basis = 'def2-tzvpd'\nmol.build()\n\n# Run DFT calculation\nmf = scf.RKS(mol)\nmf.xc = 'wB97x'\nmf.verbose = 0\nmf.run()\n\n&lt;pyscf.dft.rks.RKS at 0x7f38c01f5790&gt;\n\n\nNext, we’ll compute the full excitation spectra using the TDDFT module of PySCF. We can define how many excitations are included by the variable. The maximum is determined by the dimension of our basis set.\n\n# Run TDDFT calculation\ntd = tddft.TDDFT(mf)\ntd.nstates = 450  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\n&lt;pyscf.tdscf.rks.TDDFT at 0x7f38c00a4210&gt;\n\n\nLet’s look at how the Thomas–Reiche–Kuhn sum rule is reproduced by our spectra. The TRK sum rule states that the sum of the oscillator strenghts should equal the number of particles in the system, i.e. \n\\[\\sum_n f_n = N.\\]\n\n# Extract excitation energies (in a.u.) and oscillator strengths\nexcitation_energies = np.array(td.e)\noscillator_strengths = np.array(td.oscillator_strength(gauge='length', order=0))\n\nprint(f'Thomas-Reiche-Kuhn sum rule: {oscillator_strengths.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nThomas-Reiche-Kuhn sum rule:  8.847\nNumber of electrons: 10\n\n\nThe sum rule and the number of electrons done exactly match. We’ll come back to this a bit later. However, using the sum for the dispersion coefficients, presented above, we can compute the dispersion coefficient for the methane molecule.\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies[n]\n    for m in range(M):\n        wm = excitation_energies[m]\n        C6 += oscillator_strengths[n]*oscillator_strengths[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nDispersion coefficient:  126.347\n\n\nThe computed dispersion coefficient is excellent good agreement with the experimental value of 129.6 measured using dipole oscillator strength distribution (see https://doi.org/10.1080/00268978000103781).\nAn alternative scheme for computing the dispersion coefficient between system \\(A\\) and \\(B\\) is by the Casimir–Polder equation,\n\\[C_6^{AB} = \\frac{3}{\\pi} \\int_0^\\infty \\alpha^A(i\\omega)\\alpha^B(i\\omega)d\\omega\\]\nwhere \\(\\alpha^{A/B}(\\omega)\\) is the dynamic polarizability of system \\(A/B\\).\nWe can construct the dynamic polarizability from the oscillator strengths, using\n\\[\\alpha^A(\\omega) = \\sum \\frac{f_n^A}{\\omega_n^2-\\omega^n}.\\]\n(For more details, see the discussion by Langhoff and Karplus in The Pade Approximant in Theoretical Physics.)\n\n# Construct dynamic polarizability\nfrequencies = np.linspace(0, 30, 300)\nalpha = np.zeros(frequencies.shape)\n\nfor k in range(len(excitation_energies)):\n    alpha += oscillator_strengths[k] / (excitation_energies[k]**2 + frequencies**2) # The imaginary frequency is accounted for in the sum\n\nC6_alpha = np.trapz(alpha*alpha*3/np.pi, frequencies) # Integrate over the frequencies\nprint(C6_alpha)\n\n126.34656702984459\n\n\n\n\nTamm-Dancoff approximation\nIn TDDFT, the full excitation spectra considered contains both excitations and de-excitations. However, the excitations and de-excitations are somewhat redundant and de-excitations are often ignored to ease the computational burden. This is called the Tamm-Dancoff approximation. TDA is often regarded as a reasonable approximation that, in some instances, provides stabilization to the calculation and even improves the computed excitation spectra.\nLet’s switch the full TDDFT to TDA and see what we get.\n\n# Run TDA calculation\ntd = tddft.TDA(mf)\ntd.nstates = 350  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\nexcitation_energies_tda = np.array(td.e)\noscillator_strengths_tda = np.array(td.oscillator_strength(gauge='length', order=0))\nprint(f'Thomas-Reiche-Kuhn sum rule for TDA: {oscillator_strengths_tda.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies_tda[n]\n    for m in range(M):\n        wm = excitation_energies_tda[m]\n        C6 += oscillator_strengths_tda[n]*oscillator_strengths_tda[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nThomas-Reiche-Kuhn sum rule for TDA:  11.374\nNumber of electrons: 10\nDispersion coefficient:  107.405\n\n\nThe dispersion coefficient is well off the mark! Also the TRK sum is larger than the number of electrons in the system. Why is this?\nIf we compare the excitation spectra and the oscillator strengths for full TDDFT and with TDA, we see that while the excitation spectra from both methods agree with each other, the oscillator strenghts do not. This discrepancy results in a poor accuracy for the dispersion coefficient (and dynamic polarizability).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.tight_layout()\nax1.plot(excitation_energies, excitation_energies_tda, 'o')\nax1.set_title('Excitation energies')\nax1.set_xlabel(\"Full TDDFT\")\nax1.set_ylabel(\"TDA\")\nax2.plot(oscillator_strengths, oscillator_strengths_tda, 'o')\nax2.set_title('Oscillator strengths')\nax2.set_xlabel(\"Full TDDFT\")\nax2.set_ylabel(\"TDA\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBasis set dependence\nA diffuse basis set is in general a must when computing excited states, especially in post-Hartree-Fock theory, must also in TDDFT. If we switch to a smaller basis set (such as def2-TZVP or even def2-SVP), our estimate for the dispersion coefficient becomes significantly much poorer. Interestingly, the TRK sum rule is more closer to \\(N\\)."
  },
  {
    "objectID": "blog/blog_tddft_c6.html",
    "href": "blog/blog_tddft_c6.html",
    "title": "How to: Dispersion coefficients from TDDFT",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nDispersion and \\(C_6\\) coefficients\nDispersion interaction is a weak attractive interaction that binds even neutral, noble gas atoms to each other. Dispersion interaction sometimes mistakenly called the van der Waals (vdW) interaction, which is not correct. VdW-interactions are a combination of intermolecular interactions, where permanent molecular dipoles induce and interact with other molecular dipoles. Despite being the weakest form of van der Waals forces, dispersion interactions significantly influence the structure, stability, and properties of molecular systems, including biological molecules like proteins and DNA.\nDispersion interaction is caused by fluctuations in the electron densities in an atom or a molecule. The fluctuations cause intantaneous electric fields that are felt by ther nearby atoms or molecules, which respond to the electric field by adjusting their own electron densities accordingly. These instantaneous dipole moments then attract each other, resulting in the dispersion interaction.\nWhile other van der Waals force are well described by the computational chemistry workhorse, the density functional theory (DFT), the dispersion interaction is not. Traditional DFT methods struggle to accurately predict dispersion interaction, because the electron-electron interaction is only treated approximately, without directly accounting for the correlated motion of electrons. Instead, the dispersion is usually included in the DFT calculations as a separate correction. Using the Rayleigh–Schrödinger perturbation theory, one can show that the potential energy between two weakly interacting systems is proportional to the inverse sixth power of the distance between them, leading to the well-known\n\\[V(r) \\propto -\\frac{C_6}{r^6}\\]\nwhere \\(C_6\\) is the dispersion coefficient and \\(r\\) is the intermolecular distance.\nHere, we will have a look at how the dispersion coefficient \\(C_6\\) can be computed using time-dependent density functional theory (TD-DFT) by PySCF. We will also explore the basis set dependence of the results and how the commonly employed Tamm-Dancoff approximation (TDA) affects the results.\n\n\nOscillator strengths\nOscillator strength describes the probability for transition between quantum states when a photon is absorbed (or emitted),\n\\[f_n = \\frac{2}{3} \\omega_{n0}^A\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\]\nwhere \\(f_n\\) is the oscillator strength for a transition from the ground state to the \\(n\\)th excited state, \\(\\omega_n\\) is the corresponding transition energy and \\(\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\) is the squared magnitude for the transition dipole moment vector of the excitation.\nIn fact, one can use Rayleigh–Schrödinger perturbation theory to derive an expression for the dispersion interaction between two systems \\(A\\) and \\(B\\) separated by a large distance \\(R\\) with no overlap:\n\\[\nU_\\text{disp} = -\\frac{C_6}{R^6}\n\\]\n\\[\nC^{AB}_6 = \\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\left|\\left&lt;{\\phi_0^B|\\mathbf{r}|\\phi_m^B}\\right&gt;\\right|^2}{\\omega_{n0}^A + \\omega_{m0}^B} = \\frac{3}{2}\\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{f_{n0}^A f_{m0}^B}{\\omega_{n0}^A\\omega_{m0}^B(\\omega_{n0}^A + \\omega_{m0}^B)},\n\\]\nwhere \\(\\omega_{m0} = E_m-E_0\\).\nHere we will use the last sum to compute the dispersion coefficient \\(C_6\\) for a methane molecule.\n\n\nDispersion coefficients with TDDFT from PySCF\nWe will use PySCF to compute the excitation spectra and oscillator strenghts using time-dependent density functional theory (TD-DFT). First we’ll compute the ground state energy of a methane molecule using \\(\\omega\\)B97X exchange–correlation functional and a diffuse triple-\\(\\zeta\\) basis set def2-TZVPD.\n\nfrom pyscf import gto, scf, tdscf, dft, tddft\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define molecule and basis set\nmol = gto.Mole()\nmol.atom = 'H 0.5349075 0.1628792 0.9466881; \\\n            H 0.2074921 0.8335188 -0.6864069; \\\n            H 0.3383696 -0.9421607 -0.4547893; \\\n            H -1.0808570 -0.0543245 0.1943354; \\\n            C -0.0000122 -0.0000128 -0.0000272'\nmol.basis = 'def2-tzvpd'\nmol.build()\n\n# Run DFT calculation\nmf = scf.RKS(mol)\nmf.xc = 'wB97x'\nmf.verbose = 0\nmf.run()\n\n&lt;pyscf.dft.rks.RKS at 0x7a91b3ff8910&gt;\n\n\nNext, we’ll compute the full excitation spectra using the TDDFT module of PySCF. We can define how many excitations are included by the variable. The maximum is determined by the dimension of our basis set.\n\n# Run TDDFT calculation\ntd = tddft.TDDFT(mf)\ntd.nstates = 450  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\n&lt;pyscf.tdscf.rks.TDDFT at 0x7f38c00a4210&gt;\n\n\nLet’s look at how the Thomas–Reiche–Kuhn sum rule is reproduced by our spectra. The TRK sum rule states that the sum of the oscillator strenghts should equal the number of particles in the system, i.e. \n\\[\\sum_n f_n = N.\\]\n\n# Extract excitation energies (in a.u.) and oscillator strengths\nexcitation_energies = np.array(td.e)\noscillator_strengths = np.array(td.oscillator_strength(gauge='length', order=0))\n\nprint(f'Thomas-Reiche-Kuhn sum rule: {oscillator_strengths.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nThomas-Reiche-Kuhn sum rule:  8.847\nNumber of electrons: 10\n\n\nThe sum rule and the number of electrons done exactly match. We’ll come back to this a bit later. However, using the sum for the dispersion coefficients, presented above, we can compute the dispersion coefficient for the methane molecule.\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies[n]\n    for m in range(M):\n        wm = excitation_energies[m]\n        C6 += oscillator_strengths[n]*oscillator_strengths[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nDispersion coefficient:  126.347\n\n\nThe computed dispersion coefficient is excellent good agreement with the experimental value of 129.6 measured using dipole oscillator strength distribution (see https://doi.org/10.1080/00268978000103781).\nAn alternative scheme for computing the dispersion coefficient between system \\(A\\) and \\(B\\) is by the Casimir–Polder equation,\n\\[C_6^{AB} = \\frac{3}{\\pi} \\int_0^\\infty \\alpha^A(i\\omega)\\alpha^B(i\\omega)d\\omega\\]\nwhere \\(\\alpha^{A/B}(\\omega)\\) is the dynamic polarizability of system \\(A/B\\).\nWe can construct the dynamic polarizability from the oscillator strengths, using\n\\[\\alpha^A(\\omega) = \\sum \\frac{f_n^A}{\\omega_n^2-\\omega^n}.\\]\n(For more details, see the discussion by Langhoff and Karplus in The Pade Approximant in Theoretical Physics.)\n\n# Construct dynamic polarizability\nfrequencies = np.linspace(0, 30, 300)\nalpha = np.zeros(frequencies.shape)\n\nfor k in range(len(excitation_energies)):\n    alpha += oscillator_strengths[k] / (excitation_energies[k]**2 + frequencies**2) # The imaginary frequency is accounted for in the sum\n\nC6_alpha = np.trapz(alpha*alpha*3/np.pi, frequencies) # Integrate over the frequencies\nprint(C6_alpha)\n\n126.34656702984459\n\n\n\n\nTamm-Dancoff approximation\nIn TDDFT, the full excitation spectra considered contains both excitations and de-excitations. However, the excitations and de-excitations are somewhat redundant and de-excitations are often ignored to ease the computational burden. This is called the Tamm-Dancoff approximation. TDA is often regarded as a reasonable approximation that, in some instances, provides stabilization to the calculation and even improves the computed excitation spectra.\nLet’s switch the full TDDFT to TDA and see what we get.\n\n# Run TDA calculation\ntd = tddft.TDA(mf)\ntd.nstates = 350  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\nexcitation_energies_tda = np.array(td.e)\noscillator_strengths_tda = np.array(td.oscillator_strength(gauge='length', order=0))\nprint(f'Thomas-Reiche-Kuhn sum rule for TDA: {oscillator_strengths_tda.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies_tda[n]\n    for m in range(M):\n        wm = excitation_energies_tda[m]\n        C6 += oscillator_strengths_tda[n]*oscillator_strengths_tda[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nThomas-Reiche-Kuhn sum rule for TDA:  11.374\nNumber of electrons: 10\nDispersion coefficient:  107.405\n\n\nThe dispersion coefficient is well off the mark! Also the TRK sum is larger than the number of electrons in the system. Why is this?\nIf we compare the excitation spectra and the oscillator strengths for full TDDFT and with TDA, we see that while the excitation spectra from both methods agree with each other, the oscillator strenghts do not. This discrepancy results in a poor accuracy for the dispersion coefficient (and dynamic polarizability).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.tight_layout()\nax1.plot(excitation_energies, excitation_energies_tda, 'o')\nax1.set_title('Excitation energies')\nax1.set_xlabel(\"Full TDDFT\")\nax1.set_ylabel(\"TDA\")\nax2.plot(oscillator_strengths, oscillator_strengths_tda, 'o')\nax2.set_title('Oscillator strengths')\nax2.set_xlabel(\"Full TDDFT\")\nax2.set_ylabel(\"TDA\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBasis set dependence\nA diffuse basis set is in general a must when computing excited states, especially in post-Hartree-Fock theory, must also in TDDFT. If we switch to a smaller basis set (such as def2-TZVP or even def2-SVP), our estimate for the dispersion coefficient becomes significantly much poorer. Interestingly, the TRK sum rule is more closer to \\(N\\)."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build! Nudged elastic band method from first principles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to: Dispersion coefficients from TDDFT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to: Solvation energy for ionic species using DFT\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Quantum of Science",
    "section": "About me",
    "text": "About me\nThis is the homepage of Timo Weckman, PhD. I am a university researcher working on computational chemistry, with focus on heterogeneous catalysis, electrochemistry and surface chemistry. Here I have collected some notes and tutorials for those who come after."
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Quantum of Science",
    "section": "Publications",
    "text": "Publications\nPeer-reviewed articles and preprints. View →"
  },
  {
    "objectID": "index.html#notes-notebooks",
    "href": "index.html#notes-notebooks",
    "title": "Quantum of Science",
    "section": "Notes & Notebooks",
    "text": "Notes & Notebooks\n\nLet’s build! Nudged elastic band method from first principles\nHow to: Dispersion coefficients from TDDFT\nHow to: Solvation energy for ionic species using DFT"
  },
  {
    "objectID": "blog/blog_neb.html",
    "href": "blog/blog_neb.html",
    "title": "Let’s build! Nudged elastic band method from first principles",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nThe nudged elastic band method\nNudged elastic band method, or NEB for short, is an algorithm for search a minimum energy pathway between two point on a multi-dimensional potential energy surface (PES). The idea is to span an initial pathway on the PES between the initial and final points. Each point (or image) on the pathway is connected to one another via a spring force. The energy of the overall path (or band, since the path is essentially elastic due to the spring force) is then minimized. After optimization, the band should lie along the minimum energy pathway.\nHere, we’ll define a simple two dimensional version of the NEB by solving the minimum energy on a potential energy surface, defined as\n\\[V(x,y) = \\exp\\left[-\\cos(2\\pi x) - \\cos(\\pi (y-x))\\right] - \\exp\\left[-\\cos(2\\pi y)\\right]\\]\nWe’ll use the climbing image variant of NEB, since it ensures we find that saddle point along the minimum energy path that corresponds to the highest peak along the path.\n\n\nThe NEB procedure\nIn NEB, the idea is to minimize the energy of each image, but remove the force component along the NEB path with a spring force. Why? If we were just to minimize the energy without the spring force, all the images would slide down the potential energy surface to the nearest minimum. The spring force binds the neighbouring images together and prevents them from moving too far apart. The spring force acts only along the path defined by the images. Orthogonal to the path we use the actual force acting on the images to minimize the energy. This way the band will aling itself with the shape of the potential energy surface.\nThere are several way to define the path of the NEB band. Here we define it as the difference between the given image \\(i\\) and the neighbours before and after it depending on what is the relative energies of the images:\n\\[\\hat{\\tau}_i = \\begin{cases}\n\\tau^+_i &= \\mathbf{R}_{i+1}-\\mathbf{R}_{i} \\;,\\;\\;\\text{if} \\;E(\\mathbf{R}_{i+1}) &gt;E(\\mathbf{R}_{i}) &gt; E(\\mathbf{R}_{i-1}) \\\\\n\\tau^-_i &= \\mathbf{R}_{i}-\\mathbf{R}_{i-1} \\;,\\;\\;\\text{if} \\;E(\\mathbf{R}_{i+1}) &lt;E(\\mathbf{R}_{i}) &lt; E(\\mathbf{R}_{i-1}) \\\\\n\\end{cases}\\]\nIf the potential energy of image \\(i\\) is an extremum, the path is then \\(\\tau = \\mathbf{R}_{i+1}-\\mathbf{R}_{i-1}\\). The pathvector is normalized.\nThe forces is then divided into two orthogonal components, one parallel to the path and one orthogonal:\n\\[\\mathbf{F}_i = \\mathbf{F}_{i,\\parallel} + \\mathbf{F}_{i,\\perp}.\\]\nThe parallel path is defined as a spring force that will ensure that the images do not all just fall down the PES to the their closest minimum:\n\\[\\mathbf{F}_{i,\\parallel} = k(|\\textbf{R}_{i+1}-\\mathbf{R}_{i}| - |\\mathbf{R}_{i}-\\mathbf{R}_{i-1}|)\\hat{\\tau}_i,\\]\nwhere \\(k\\) is the spring constant for the harmonic force. The orthogonal component is obtained from the the ‘real’ force acting on the system, i.e. the force we get as the gradient of the potential energy:\n\\[\\mathbf{F} = -\\nabla V(\\mathbf{R}_i).\\]\nTo retain only the orthogonal component to the path vector \\(\\tau_i\\), we need to remove the projection of the force parallel to the path:\n\\[\\mathbf{F}_{i,\\perp} = -\\nabla V(\\mathbf{R}_i) + \\left[\\nabla V(\\mathbf{R}_i)\\cdot \\hat{\\tau}_i \\right] \\hat{\\tau}_i.\\]\nIn the climbing image (CI) NEB, the image with the highest energy is not subjest to the spring force. Instead, it will try to climb up the potential energy surface instead of going down. This means that the component parallel to the NEB path will be reversed:\n\\[\\mathbf{F}_{i, \\text{max}} = -\\nabla V(\\mathbf{R}_{i, \\text{max}}) + \\left[2\\nabla V(\\mathbf{R}_{i, \\text{max}})\\cdot \\hat{\\tau}_{i, \\text{max}}\\right] \\hat{\\tau}_{i, \\text{max}}.\\]\n\n\nBuilding NEB for a 2D potential\nWe’ll use the steepest descent method with a fixed step size to minimize the energy of the band. In steepest descent, we follow the direction of the steepest descent (i.e. the force vector) and update the coordinates of each image consecutively using a fixed step size.\nIn addition to Numpy, we’ll use Sympy library for symbolic calculation to do the partial derivatives of the potential function. Let’s begin by defining the potential energy surface for our system.\n\nimport sympy as sp\nfrom IPython.display import display\n\n# Define symbolic variables\nx, y = sp.symbols('x y')\n# Define the function V(x, y)\nV_sp = sp.exp(-sp.cos(2*sp.pi*x) - sp.cos(sp.pi*(y-x))) - sp.exp(-sp.cos(sp.pi*y))\n\nNext, we’ll compute the partial derivatives of the potential energy function and convert the experssions into a separate functions using the lambdify-function. We can then readily use these functions in our NEB procedure. We can use the latex-function in Sympy to print out Latex expressions for our partial derivatives. A similar python or print_python function would give an expression in Python that could then be defined separately.\n\n# Compute the partial derivatives using Sympy\ndVx_sp = sp.diff(V_sp, x)\ndVy_sp = sp.diff(V_sp, y)\n# Lambdify the expressions\nV = sp.lambdify((x, y), V_sp, 'numpy')\ndVx = sp.lambdify((x, y), dVx_sp, 'numpy')\ndVy = sp.lambdify((x, y), dVy_sp, 'numpy')\n\nprint(sp.latex(dVx_sp))\nprint(sp.latex(dVy_sp))\n\n\\left(2 \\pi \\sin{\\left(2 \\pi x \\right)} - \\pi \\sin{\\left(\\pi \\left(- x + y\\right) \\right)}\\right) e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}}\n\\pi e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}} \\sin{\\left(\\pi \\left(- x + y\\right) \\right)} - \\pi e^{- \\cos{\\left(\\pi y \\right)}} \\sin{\\left(\\pi y \\right)}\n\n\nThe partial derivatives are then:\n\\[\\frac{\\partial V}{\\partial x} = \\left(2 \\pi \\sin{\\left(2 \\pi x \\right)} - \\pi \\sin{\\left(\\pi \\left(- x + y\\right) \\right)}\\right) e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}}\\]\n\\[\\frac{\\partial V}{\\partial y} = \\pi e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}} \\sin{\\left(\\pi \\left(- x + y\\right) \\right)} - \\pi e^{- \\cos{\\left(\\pi y \\right)}} \\sin{\\left(\\pi y \\right)}\\]\nFinally, let’s plot the potential energy surface as a contour plot. The points \\([0,1]\\) and \\([1,1]\\) are two minima on the PES. We’ll be using these as the initial and final points, respectively. We’ll mark these two points down using a red dot.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-0.6, 1.4, 40)\ny = np.linspace(0.5, 1.4, 40)\nX, Y = np.meshgrid(x, y)\nZ = V(X,Y)\n\nfig, ax = plt.subplots()\nax.contour(X, Y, Z, 30, colors='black', linestyles='solid')\nax.scatter([0,1], [1,1], color='r')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Potential energy surface')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll begin by defining an initial guess for the minimum energy path. A good initial guess is crucial for the accuracy of the minimum energy path, since we really only relax the initial guess and do not explore the potential energy surface beyond our guess. A linear interpolation between the initial and final states is a common approximation.\n\n# Initial state (0,1)\n# Final state (1,1)\n\nnimg = 16 # Number of images\ninit = np.array([0,1]) # Initial state on the PES\nfinal = np.array([1,1]) # Final state on the PES\n\n# Interpolate path, save full path to images\nimages = [init]\nfor img in range(1,nimg+1):\n    images.append(init - (init-final) / (nimg+1) * (img))\nimages.append(final)\nimages = np.array(images)\n\nNext, we’ll initialize the forces and energies of the system. The forces will be used in our steepest descent optimization scheme and the energies are used for defining the path vector and for choosing the climbing image, each cycle.\n\n# List of forces on all images (init and final states will remain stationary)\nforces = np.zeros((len(images),2))\nenergies = np.zeros((len(images),))\n# Compute energies for the initial and final states\nenergies[0] = V(images[0,0], images[0,1])\nenergies[-1] = V(images[-1,0], images[-1,1])\n\nNext, we’ll define the NEB procedure. First, we’ll evaluate the energy of each image and then loop over all the images in our band and, 1. Construct the path vector 2. Normalize the path vector 3. Compute the real force 4. Compute the spring force 5. Apply the forces in the optimization\n\n# NEB parameters\nstep_size = 0.01 # Step size for optimisation1\nk = 0.005 # Spring constant for the harmonic force\n\ninitial_images = images.copy() # Copy the initial guess for the path\nreal_force = np.zeros((2,))\npath_history = [images.copy()] # Record the pathway from beginning to end, for plotting purposes\nfor step in range(70): # Run the optimisation for some fixed amount of steps\n    for i in range(1, len(images)-1):\n        x, y = images[i]\n        energies[i] = V(x,y)\n    for i in range(1, len(images)-1):\n        x, y = images[i]\n\n        # Construct the path vector\n        if energies[i] &gt; energies[i-1] and energies[i] &lt; energies[i+1]:\n            path = images[i+1] -  images[i]\n        elif energies[i] &lt; energies[i-1] and energies[i] &gt; energies[i+1]:\n            path = images[i] -  images[i-1]\n        else:\n            path = images[i+1] - images[i-1]\n        path /= np.linalg.norm(path) # Normalize the path vector\n\n        real_force = -np.array([dVx(x,y), dVy(x,y)]) # Compute the real force\n        spring_force = k * (np.linalg.norm(images[i+1]-images[i]) - np.linalg.norm(images[i]-images[i-1])) * path # Compute the spring force\n        # Steepest descent optimization\n        if energies[i] == np.max(energies): # If the image has the highest potential energy, use the climbing image procedure\n            forces[i] = real_force - 2 * np.dot(real_force, path) * path\n        else: # Else, use only the orthogonal component of the real force and the spring force\n            forces[i] = real_force - np.dot(real_force, path) * path + spring_force\n        \n    if np.max(np.sum(forces**2, axis=1)**0.5) &lt; 0.05:\n        break\n\n    images[1:-1] += step_size * forces[1:-1] / np.linalg.norm(forces[1:-1]) # Steepest descent optimisation\n    path_history.append(images.copy())\n\nxopt, yopt = images[energies == np.max(energies)][0]\neopt = energies[energies == np.max(energies)][0]\nprint(f\"NEB saddlepoint at ({xopt: .3f}, {yopt: .3f}) with energy {eopt: 0.7f}\")\nprint(f\"Exact saddlepoint at (0.427, 0.777) with energy {V(0.427, 0.777): 0.7f}\")\n\nNEB saddlepoint at ( 0.427,  0.775) with energy -0.5910029\nExact saddlepoint at (0.427, 0.777) with energy -0.5910308\n\n\nWe find a saddle point very close to an exact solution. Let’s plot our initial and the optimized NEB paths on the potential energy surface.\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-0.4, 1.2, 40)\ny = np.linspace(0.7, 1.2, 40)\nX, Y = np.meshgrid(x, y)\nZ = V(X,Y)\n\nfig, ax = plt.subplots()\nax.contour(X, Y, Z, 40, colors='black', linestyles='solid')\nax.scatter(initial_images[:,0], initial_images[:,1], color='r')\nax.scatter(images[:,0], images[:,1], cmap='binary', color='black')\nax.set_xlabel('x')\nax.set_ylabel('y')\n#fig.savefig('plot.png')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s animate the path to see how the band relaxes onto the minimum energy path.\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n# Ensure that animations are rendered inline\n%matplotlib inline\n\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z, 40, colors='black', linestyles='solid')\nscatter_initial = ax.scatter([], [], color='r')\nscatter_images = ax.scatter([], [], color='black')\nplt.close(fig)\n\ndef data_from_history(iteration):\n    if iteration &lt; len(path_history):\n        return path_history[0], path_history[iteration]\n    else:\n        return path_history[0], path_history[-1]\n\n# Update function for animation\ndef update(frame):\n    initial_images, images = data_from_history(frame)\n    scatter_initial.set_offsets(initial_images)\n    scatter_images.set_offsets(images)\n    return scatter_initial, scatter_images,\n\n# Create animation\nani = FuncAnimation(fig, update, frames=range(80), blit=True)\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe initial movement is largest where the gradient is largest (the points on top of the high energy ridge). The highest energy point slides down towards the saddle point. After the gradient acting on the highest energy images decreases, all the point start to relax. The spacing between the points is not uniform. Where the potential energy is relatively flat, the points have larger freedom of movement and align themselves with the contours of the potential energy surface with larger spacing between the points."
  },
  {
    "objectID": "blog/blog_compute_solvation.html",
    "href": "blog/blog_compute_solvation.html",
    "title": "How to: Solvation energy for ionic species using DFT",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nCalculating solvation free energies\nThe dielectric continuum solvent models facilitate the modelling of chemical reactions is condensed phases. The dielectric continuum models the effect of the solvent and can be used to compute the solvation free energy of a species. The solvation free energy \\(\\Delta G_\\text{solv}\\) can be added to the accurate gas-phase free energy values to obtain the corresponding solution-phase free energy:\n\\[G_\\text{soln} = G_\\text{gas} + \\Delta G_\\text{solv} + RT \\ln \\left( \\frac{RT}{P} \\right)\\]\nwhere the last term converts the gas-phase standard state to teh solution-phase standard state of 1 M.\nHowever, there are a few points one needs to address when the continuum solvent models are applied (see https://doi.org/10.1021/jp107136j). The solvation free energy can be computed from the difference between the energies obtained in gas-phase and in the dielectric continuum solvent,\n\\[\\Delta G_\\text{solv} = (E_\\text{soln} + G_\\text{nes}) - E_\\text{gas}\\]\nwhere \\(E_\\text{soln}\\) and $ E_$ are the electronic energies of the solute with and without the continuum solvent field and \\(G_\\text{nes}\\) refers to the sum of any nonelectrostatic contributions to the solvation free energy, such as cavitation and dispersion-repulsion interactions. The main point here is that the solvation free energy is obtained from the difference in electronic energies, not a difference in free energies computed in solution- and gas-phases. This is because of the way the continuum models are parametrized and from the fact that the ideal gas models used to compute the free energies only apply in the gas-phase.\nSo, in order to calculate all the components of our thermodynamic cycle we will: 1. Optimize all the species in the upper-rung of the thermodynamic cycle in gas-phase 2. Compute the total energy and all the free energy terms for these species 3. Optimize all the species in the lower-rung of the thermodynamic cycle with the dielectric continuum solvent model\nWe will then compute the free energy for the reaction based on the gas-phase free energies,\n\\[\\Delta G_\\text{bind, g} = \\sum_i \\nu_i G_\\text{i} - \\sum_a \\nu_a G_a\\]\nwhere \\(i\\) and \\(a\\) correspond to product and reactant species, respectively.\nThe solvation energies are then computed using the electronic energies of the species (not including the enthalpy and entropy-terms used in gas-phase calculation). Often, especially for ionic systems, it is preferred to employ a hybrid solvation model, that is to use both explicit and implicit solvent together. This will signicantly improve the accuracy of our solvation energies.\n\nfrom IPython.display import display, Image\ndisplay(Image(filename='scheme1.png'))\n\n\n\n\n\n\n\n\n\n\nExample calculations using PySCF\nLet’s proceed step-by-step using the PySCF and calculate the solvation energy of copper(II)-ion.\n\nImplicit solvent only\nWe’ll define a simple system consisting of the copper-ion in gas-phase and with implicit solvent only.\n\nfrom pyscf import gto, scf, solvent\n\nmol = gto.Mole()\nmol.atom = 'Cu 0 0 0' \nmol.basis = 'def2-svp' \nmol.charge = 2 \nmol.spin = 1\nmol.build()\n\nmf = scf.UKS(mol)\nmf.xc = 'PBE'\nmf.verbose = 0\nmf.run()\n\n# Add implicit solvent\nmf_solvent = solvent.ddCOSMO(mf)\nmf_solvent.with_solvent.eps = 78.39\nmf_solvent.verbose = 0\nmf_solvent.mo_coeff = mf.mo_coeff\nmf_solvent.run()\n\nprint(f\"Total energy in vacuum: {mf.e_tot: .3f} Eh\")\nprint(f\"Total energy in solvent: {mf_solvent.e_tot: .3f} Eh\")\nprint(f\"Solvation free energy: {(mf_solvent.e_tot - mf.e_tot) * 627.5: .3f} kcal/mol\")\n\nTotal energy in vacuum: -1638.931 Eh\nTotal energy in solvent: -1639.475 Eh\nSolvation free energy: -341.570 kcal/mol\n\n\nThe evaluated solvation energy is quite off the mark, considering that the solvation free is estimated to be about -500 kcal/mol (see https://pubs.acs.org/doi/abs/10.1021/acs.jpca.8b06674).\n\n\nHybrid solvation model\nLet’s embed the copper-ion in a solvation shell formed by water molecules and compute the solvation free energy using the thermodynamic cycle discussed above. This is considerably more involved calculation, since we need to compute the Hessian for the gas-phase species.\nHow do we obtain a structure for the solvation shell? This is a good question. Some might be found from the literature, at least for a similar ion. An alternative approach would be to construct one using a low-level model, e.g. density functional tight binding (DFTB) or similar approach. Here, we will adopt an artificially small cluster with four water molecules. There are however larger solvation structures available from the literature (see e.g. https://pubs.acs.org/doi/10.1021/jp804373p).\n\n\nWater cluster structures\nConsider the two water clusters, with and without a copper-ion, preoptimized using PBE and def2-SVP basis set.\nWe will use these clusters to calculate the gas-phase reaction for the binding of the copper-ion with the solvent molecules. We will also use them to calculate the solvation energy for each cluster. The solvated structure should be optimized, but we will neglect this step here since the calculations, albeit with a small basis set, are already quite cumbersome to run on a laptop/desktop.\n\ndisplay(Image(filename='solvationcluster.png'))\n\n\n\n\n\n\n\n\n\nfrom pyscf.hessian import thermo\n\n# First, compute copper with the 'solvation cluster'\ncoords = '''\n  Cu  0.00770670007957     -0.01075714803486      0.00684346198661\n  O   1.62629858055350      0.59958796551543     -0.96137824606612\n  H   2.38274772811028      0.05517739425091     -1.27095980213712\n  H   1.63486316135084      1.44991263891621     -1.45427267152436\n  O   -0.08513692098164      1.85563301721865      0.70772230089222\n  H   -0.92091296008101      2.27895856953297      1.00777628197219\n  H   0.63973505010478      2.22714364462856      1.26089012590088\n  O   -0.16307318602147     -1.71475219698127     -1.01669913168950\n  H   0.12868744546777     -1.79759035478399     -1.95233645651861\n  H   -0.00508273379089     -2.58677644056665     -0.58749562922375\n  O   -1.47600452153588     -0.67910102216609      1.13003992630434\n  H   -1.68821978806555     -0.41530609819069      2.05174002936686\n  H   -2.28219008519028     -1.07343020933916      0.72891087073636\n'''\n\ntemperature = 298.15  # Standard temperature in K\npressure = 101325  # Standard pressure in Pa\n\nmol_cluster = gto.M(atom=coords, basis='def2-svp', charge=2, spin=1)\nmf_cluster = scf.UKS(mol_cluster)\nmf_cluster.xc = 'PBE'\nmf_cluster.verbose = 0\nmf_cluster.run()\n\nhessian = mf_cluster.Hessian().kernel()\n# Frequency analysis\nfreq_cluster = thermo.harmonic_analysis(mf_cluster.mol, hessian)\nthermo_cluster = thermo.thermo(mf_cluster, freq_cluster['freq_au'], temperature, pressure)\n\n\n# Second, compute just the 'solvation cluster'\ncoords = '''\n  O   1.59569157211650      0.52671408407826      0.55875125592345\n  H   1.22572269113165      0.40613430342645     -0.37753454653395\n  H   1.65562959781890     -0.39080578496591      0.88476545887585\n  O   -1.80219894908837      1.36621874756072     -0.76471975545405\n  H   -1.46831586426498      1.39336098953180      0.19232417112175\n  H   -1.87403013018684      2.30887164269416     -1.00164540312905\n  O   0.36899920133373      0.20801276276540     -1.77155683816915\n  H   0.69644080847323      0.81096729919872     -2.46358962449703\n  O   -0.64414560352736      1.46201234097889      1.62413639246884\n  H   -0.47158383660499      2.36988722572480      1.93233540537252\n  H   0.25833209306559      1.14749559670315      1.28200372985479\n  H   -0.51999897026706      0.60946637230353     -1.49946089583396\n'''\n\nmol_solvent = gto.M(atom=coords, basis='def2-svp', charge=0, spin=0)\nmf_solvent = scf.UKS(mol_solvent)\nmf_solvent.xc = 'PBE'\nmf_solvent.verbose = 0\nmf_solvent.run()\n\nhessian = mf_solvent.Hessian().kernel()\n# Frequency analysis\nfreq_solvent = thermo.harmonic_analysis(mf_solvent.mol, hessian)\nthermo_solvent = thermo.thermo(mf_solvent, freq_solvent['freq_au'], temperature, pressure)\n\n\n# Third, compute just the copper ion separately\ncoords = '''\n  Cu   0.0      0.0      0.0\n'''\n\nmol_cu = gto.M(atom=coords, basis='def2-svp', charge=2, spin=1)\nmf_cu = scf.UKS(mol_cu)\nmf_cu.xc = 'PBE'\nmf_cu.verbose = 0\nmf_cu.run()\n\n&lt;pyscf.dft.uks.UKS at 0x7fa11f128610&gt;\n\n\nFor the gas-phase copper-ion, we need to compute the translational entropy using the Sackur-Tetrode equation,\n\\[S_\\text{trans} = R \\left( \\ln \\left( \\frac{kT}{P} \\left( \\frac{2\\pi mkT}{h^2} \\right)^{\\frac{3}{2}} \\right) + \\frac{5}{2} \\right)\n\\]\n\nfrom pyscf.data import nist\nimport numpy as np\n\n# Conversion factors from PySCF's NIST database\nkB = nist.BOLTZMANN\nh = nist.PLANCK\n# Gas constant in units of Hartree\nR_Eh = kB*nist.AVOGADRO / (nist.HARTREE2J * nist.AVOGADRO)\n# Mass of the atom\nmass_tot = np.sum(mol_cu.atom_mass_list()) * nist.ATOMIC_MASS\n# Sackur-Tetrode equation for the translational entropy\nS_trans = R_Eh * (np.log((2.0 * np.pi * mass * kB * temperature / h**2)**1.5 * kB * temperature / pressure) + 5/2)\nthermo_cu = mf_cu.e_tot - temperature * S_trans\n\n\ndGbind_gas = (thermo_cluster['G_tot'][0] - thermo_solvent['G_tot'][0] - thermo_cu) * 627.5\nprint(f\"Gibbs free energy for the gas-phase reaction: {dGbind_gas: .3f} kcal/mol\")\n\nGibbs free energy for the gas-phase reaction: -299.312 kcal/mol\n\n\nThe solvation energies are computed from the electronic energies between the gas-phase and solvated molecules.\n\n# Add implicit solvent to cluster and solvent\nmf_cluster_solvation = solvent.ddCOSMO(mf_cluster)\nmf_cluster_solvation.with_solvent.eps = 78.39\nmf_cluster_solvation.verbose = 0\nmf_cluster_solvation.mo_coeff = mf_cluster.mo_coeff\nmf_cluster_solvation.run()\n\nmf_solvent_solvation = solvent.ddCOSMO(mf_solvent)\nmf_solvent_solvation.with_solvent.eps = 78.39\nmf_solvent_solvation.verbose = 0\nmf_solvent_solvation.mo_coeff = mf_solvent.mo_coeff\nmf_solvent_solvation.run()\n\ndGsolv_cluster = (mf_cluster_solvation.e_tot - mf_cluster.e_tot) * 627.5\ndGsolv_solvent = (mf_solvent_solvation.e_tot - mf_solvent.e_tot) * 627.5\nprint(f\"Solvations free energies for the cluster: {dGsolv_cluster: .3f} kcal/mol\")\nprint(f\"Solvations free energies for the solvent: {dGsolv_solvent: .3f} kcal/mol\")\n\nSolvations free energies for the cluster: -202.658 kcal/mol\nSolvations free energies for the solvent: -8.953 kcal/mol\n\n\nWe also need to account for the change in the standard state when going from gas-phase to the solution phase:\n\nprint(f\"Solvation free energy of copper(II) in water: {dGbind_gas + dGsolv_cluster - dGsolv_solvent: .3f} kcal/mol\")\n\nSolvation free energy of copper(II) in water: -493.016 kcal/mol\n\n\nThis gets very close to the experimental value, although we have performed only very simple calculations with DFT+GGA, no van der Waals correction and only a small basis set. Using more accurate setup and a larger solvation cluster should improve on this. The convergence of the results with respect to the basis set used and the size of the solvation cluster should be checked."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nAbout this site"
  }
]