[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Quantum of Science",
    "section": "",
    "text": "Peer-Reviewed Publications\n\n\nThis page lists peer-reviewed journal articles. Author order and formatting follow the published versions.\n\n\n\nJournal Articles\n\n\n\n Anže Zupanc, Joseph Install, Timo Weckman, Marko M. Melander, Marianna Kemell, Karoliina Honkala, Timo Juhani Repo ,  “Sustainable urban mining of silver with fatty acids” , Chemical Engineering Journal 512, 162129 (2025).\n\n\n Timo Weckman ,  “Dispersion with Fixed Diagonal Matrices: Exchange energy correction and an assessment of the Becke–Roussel exchange hole” , Journal of Chemical Physics 162(5), 054112 (2025).\n\n\n Sherif Hegazy, Hanan H. Ibrahim, Timo Weckman, Tao Hu, Sari Tuomikoski, Ulla Lassi, Karoliina Honkala, Varsha Srivastava ,  “Synergistic pyrolysis of Cellulose/Fe-MOF composite: A combined experimental and DFT study on dye removal” , Chemical Engineering Journal 504, 158654 (2024).\n\n\n Jayanta Dana, M. R. Ajayakumar, Alexander Efimov, Timo Weckman, Karoliina Honkala, Nikolai V. Tkachenko ,  “Structure-dependent activation of a Co molecular catalyst through photoinduced electron transfer from a CdTe quantum dot” , Nanoscale 16(44), 20725–20737 (2024).\n\n\n Mario Mäkinen, Timo Weckman, Kari Laasonen ,  “Modelling the growth reaction pathways of zincone ALD/MLD hybrid thin films: A DFT study” , Physical Chemistry Chemical Physics 26(24), 17334–17344 (2024).\n\n\n Hanan Ibrahim, Timo Weckman, Dmitry Murzin, Karoliina Honkala ,  “Understanding selective hydrogenation of phenylacetylene on PdAg single-atom alloys: DFT insights on molecule size and surface ensemble effects” , Journal of Catalysis 434, 115523 (2024).\n\n\n Anže Zupanc, Joseph Install, Timo Weckman, Marko M. Melander, Mikko J. Heikkilä, Marianna Kemell, Karoliina Honkala, Timo Juhani Repo ,  “Sequential selective dissolution of coinage metals in recyclable ionic media” , Angewandte Chemie 136(31), e202407147 (2024).\n\n\n Jayanta Dana, Ramsha Khan, Timo Weckman, Karoliina Honkala, Nikolai V. Tkachenko ,  “Laterally bound Co porphyrin on CdTe quantum dots: A long-lived charge-separated nanocomposite” , Journal of Physical Chemistry C 127(21), 10164–10173 (2024).\n\n\n Marko M. Melander, Tongwei Wu, Timo Weckman, Karoliina Honkala ,  “Constant inner potential DFT for modelling electrochemical systems under constant potential and bias” , npj Computational Materials 10(1), 5 (2023).\n\n\n Luca Mastroianni, Timo Weckman, Kari Eränen, Vincenzo Russo, Dmitry Yu. Murzin, Karoliina Honkala, Tapio Salmi ,  “Oxidative dehydrogenation of alcohols on gold: An experimental and computational study on the role of water and alcohol chain length” , Journal of Catalysis 425, 233–244 (2023).\n\n\n Derk Kooi, Timo Weckman, Paola Gori-Giorgi ,  “Dispersion without many-body density distortion: Assessment on atoms and small molecules” , Journal of Chemical Theory and Computation 17(4), 2283–2293 (2021).\n\n\n Timo Weckman, Mahdi Shirazi, Simon D. Elliott, Kari Laasonen ,  “Kinetic Monte Carlo study of the atomic layer deposition of zinc oxide” , Journal of Physical Chemistry C 122(47), 27044–27058 (2018).\n\n\n Timo Weckman, Kari Laasonen ,  “Atomic layer deposition of zinc oxide: Study on the water pulse reactions from first principles” , Journal of Physical Chemistry C 122(14), 7685–7694 (2018).\n\n\n Li Ma, Marko M. Melander, Timo Weckman, Kari Laasonen, Jaakko Ahola ,  “CO oxidation on the Au15Cu15 cluster and the role of vacancies in the MgO(100) support” , Journal of Physical Chemistry C 120(47), 26747–26758 (2016).\n\n\n Timo Weckman, Kari Laasonen ,  “Atomic layer deposition of zinc oxide: Diethyl zinc reactions and surface saturation from first principles” , Journal of Physical Chemistry C 120(38), 21460–21471 (2016).\n\n\n Li Ma, Marko M. Melander, Timo Weckman, Saana Lipasti, Jaakko Ahola, Kari Laasonen ,  “DFT simulations and microkinetic modelling of 1-pentyne hydrogenation on Cu20 model catalysts” , Journal of Molecular Graphics and Modelling 65, 61–170 (2016).\n\n\n Timo Weckman, Kari Laasonen ,  “First-principles study of the atomic layer deposition of alumina by the TMA–H2O process” , Physical Chemistry Chemical Physics 17(26), 17322–17334 (2015).\n\n\n Maryam Borghei, Gianmario Scotti, Petri Kanninen, Timo Weckman, Ilya V. Anoshkin, Albert G. Nasibulin, Sami Franssila, Esko I. Kauppinen, Tanja Kallio, Virginia Ruiz ,  “Enhanced performance of a silicon microfabricated direct methanol fuel cell with PtRu catalysts supported on few-walled carbon nanotubes” , Energy 65, 612–620 (2014).\n\n&lt;/ol&gt;\n\n\nAn up-to-date list, including links to the publciations can be found on Google Scholar.\n\n\n\n\n \n\n    © 2026 Timo Weckman"
  },
  {
    "objectID": "blog_index.html",
    "href": "blog_index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build! Nudged elastic band method from first principles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build Hartree-Fock part 4: self-consistent field\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build Hartree-Fock part 3: Roothan-Hall equations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build Hartree-Fock part 2: orthogonalization procedures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build Hartree-Fock part 1: goals and principles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to: Solvation energy for ionic species using DFT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to: Dispersion coefficients from TDDFT\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/blog_buildhf_part1.html",
    "href": "blog/blog_buildhf_part1.html",
    "title": "Let’s build Hartree-Fock part 1: goals and principles",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nWhat is our goal: diagonalizing the Hamiltonian\nThe Schrödinger equation is essentially an eigenvalue problem. This is true, whether or not the system consists of a single particle or multiple particles interacting with one another. Once we write down the Hamiltonian as a matrix (in a finite-dimensional case, obviously), we can solve all the quantum states by diagonalizing the Hamiltonian, i.e. solving the eigenvalues and -vectors of the Hamiltonian matrix. The ground state is the state with the lowest eigenvalue, with all the other states being excited states.\nLet’s consider a numerical solution to a simple one-particle system: the quantum mechanical harmonic oscillator. We will be using the dimensionless form of the equation, i.e. with energy will be given in units of \\(\\hbar\\omega\\) and distance in units of \\(\\sqrt{\\hbar/(m\\omega)}\\). The Schrödinger equation for the one-dimensional harmonic oscillator will then be\n\\[\n    \\left(-\\frac{1}{2} \\frac{d^2}{dx^2} + \\frac{1}{2}x^2 \\right)\\Psi_n(x) = E_n\\Psi_n(x),\n\\]\nwhere the \\(\\Psi_n\\) is the wave function of the nth state and \\(E_n\\) the corresponding energy. The Hamiltonian operator \\(\\hat{H}\\) consists of two terms: the kinetic energy \\(-\\frac{1}{2} \\frac{d^2}{dx^2}\\) and the potential energy \\(\\frac{1}{2}x^2\\),\n\\[\n    \\hat{H} = -\\frac{1}{2} \\frac{d^2}{dx^2} + \\frac{1}{2}x^2.\n\\]\nWe’ll now discretize both of these operators on a real-space grid and solve the harmonic oscillator numerically. We’ll then compare our results with the well known analytical solutions.\n\n\nHow to discretize the Hamiltonian\nTo discretize the Hamiltonian, let’s first span our one-dimensional space on a grid \\(\\mathbf{x} = [x_0, x_1, \\ldots x_N]\\) where \\(\\{x_i\\}\\) is the \\(i\\)th grid point on an interval \\([-4,4]\\), with each grid point equally spaced. The kinetic energy part of the Hamiltonian can then be expressed as a finite difference matrix,\n\\[\n    \\frac{d^2 f}{dx^2} \\approx \\frac{f(x_{n+1}) - 2f(x_n) - f(x_{n-1})}{h^2}\n\\]\nwhere \\(h\\) is the spacing between the grid points. This type of matrix looks like\n\\[\nD_{xx} = \\frac{1}{h^2}\\left[\n    \\begin{array}{cccccc}\n    -2 & 1 & 0 & \\ldots & 0 & 0 \\\\\n    1 & -2 & 1 & \\ldots & 0 & 0 \\\\\n    0 & 1 & -2 & \\ldots & 0 & 0 \\\\\n    \\vdots & & &\\ddots & & \\vdots \\\\\n    0& 0& 0 & \\ldots & -2 & 1 \\\\\n    0 & 0 & 0 & \\ldots &1 & -2\n    \\end{array}\n\\right]\n\\]\nWe’ll construct a matrix of this type using NumPy:\n\nimport numpy as np\n\nh = 0.1\nx = np.arange(-4, 4, h)\nn = len(x)\n\n# Discretized Laplacian\nDxx = -2 * np.eye(n,)\nb = np.ones(n)\nDxx += np.diag(b[:-1], k=1) + np.diag(b[:-1], k=-1)\nDxx /= h**2\n\nTo construct the full Hamiltonian, we also need to discretize the the harmonic potential. We write the potential as a diagonal matrix, with the potential evaluated on each grid point on the diagonal.\n\n# Harmonic potential\nV = np.zeros((n,n))\nfor i in range(n):\n    V[i,i] = 0.5 * x[i]**2\n\n\n\nSolving the Schrödinger equation\nCombining the kinetic and potential energy matrices results in the full Hamiltonian for the 1-dimensional harmonic oscillator. Solving the Schrödinger equation now simply means diagonalizating the Hamiltonian matrix. This gives us the eigenvalues and -vectors of the system.\n\nH = -0.5 * Dxx + V\ne, wf = np.linalg.eigh(H)\n\nThe ground state is the eigenstate corresponding to the lowest eigenvalue. The Numpy eigh-function already sorts the eigenstate in an ascending order of the eigenvalues, so the ground state corresponds to the first entry on the list. The eigenvectors are spanned on the real space grid and so we can plot it on the grid. However, the wavefunction needs to be correctly normalized. The norm of the wavefunction can be computed on the grid as\n\\[\n    \\int_{-\\infty}^\\infty \\psi(x)^2 dx \\approx \\sum_i \\left|\\psi(x_i)\\right|^2 h = 1,\n\\]\nand so we need to account for the grid spacing in our eigenvectors by dividing them with the square root of the grid spacing. Let’s compare our numerical solution with the exact analytical solution on the same grid.\nLet’s see how well the ground state and the first two excited states measure against the analytical solutions.\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(12, 12))\nfig.tight_layout(pad=1.0)\n\ne0 = e[0]\nwf0 = wf[:,0] / h**0.5\nwf0_analytical = np.pi**-0.25 * np.exp(-0.5*x**2)\nprint(f'Numerical ground state energy {e0:.4f}')\nprint(f'Exact ground state energy 0.5000')\naxs[0].plot(x, wf0, label='Numerical solution')\naxs[0].plot(x, wf0_analytical, linestyle='dashed', label='Analytical solution')\naxs[0].set_title('Ground state')\naxs[0].set_aspect('equal')\naxs[0].set_xlim(-4,4)\naxs[0].set_ylim(0, 1.)\naxs[0].legend(loc=\"upper right\")\n\nwf1 = wf[:,1] / h**0.5\nwf1_analytical = np.pi**-0.25 * np.exp(-0.5*x**2) * (2**0.5 * x)\naxs[1].plot(x, wf1, label='Numerical solution')\naxs[1].plot(x, wf1_analytical, linestyle='dashed', label='Analytical solution')\naxs[1].set_title('1st excited state')\naxs[1].set_aspect('equal')\naxs[1].set_xlim(-4,4)\naxs[1].set_ylim(-1, 1.)\naxs[1].legend(loc=\"upper right\")\n\nwf2 = wf[:,2] / h**0.5\nwf2_analytical = -np.pi**-0.25 * np.exp(-0.5*x**2) * (1 / (2**1.5)) * (4*x**2 - 2)\naxs[2].plot(x, wf2, label='Numerical solution')\naxs[2].plot(x, wf2_analytical, linestyle='dashed', label='Analytical solution')\naxs[2].set_title('2nd excited state')\naxs[2].set_aspect('equal')\naxs[2].set_xlim(-4,4)\naxs[2].set_ylim(-1, 1.)\naxs[2].legend(loc=\"upper right\")\nplt.show()\n\nNumerical ground state energy 0.4997\nExact ground state energy 0.5000\n\n\n\n\n\n\n\n\n\nOur results on the numerical grid nicely match with the analytical solutions. For the second excited state, our numerical result is identical up to a phase-factor, which we can fix by multiplying the analytical solution by -1. This will not affect any observable results, since the phase-factor will alway cancel out when we are computing observables.\n\n\nConclusion\nRegardless whether the quantum system consists of a single particle or many particles, the system is solved by constructing the Hamiltonian and then diagonalizing it. This is essentially the goal in the Hartree–Fock approach as well. However, with many particle systems we face the issue of electron–electron interaction. We will therefore first construct a sort of approximate Hamiltonian that we will solve iteratively (using the self-consistent field). We will then use the eigenstates of the Hartee–Fock solution to build the exact Hamiltonian (configuration interaction). It must be emphasized that in the Hartree–Fock method, we make no approximations to the Hamiltonian per se. The Hartree–Fock approximation is to write the antisymmetric wavefunction in terms of only a single Slater determinant. This results in some of the electron–electron integrals to become zero, effectively resulting in an “incomplete” Hamiltonian, called the Fockian matrix.\n\n\nQuestions\n\nHow does changing the grid spacing affect the accuracy of the results?\nTry changing the potential function and solve the Schrödinger equation for a different system."
  },
  {
    "objectID": "blog/blog_compute_solvation.html",
    "href": "blog/blog_compute_solvation.html",
    "title": "How to: Solvation energy for ionic species using DFT",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nCalculating solvation free energies\nThe dielectric continuum solvent models facilitate the modelling of chemical reactions is condensed phases. The dielectric continuum models the effect of the solvent and can be used to compute the solvation free energy of a species. The solvation free energy \\(\\Delta G_\\text{solv}\\) can be added to the accurate gas-phase free energy values to obtain the corresponding solution-phase free energy:\n\\[G_\\text{soln} = G_\\text{gas} + \\Delta G_\\text{solv} + RT \\ln \\left( \\frac{RT}{P} \\right)\\]\nwhere the last term converts the gas-phase standard state to teh solution-phase standard state of 1 M.\nHowever, there are a few points one needs to address when the continuum solvent models are applied (see https://doi.org/10.1021/jp107136j). The solvation free energy can be computed from the difference between the energies obtained in gas-phase and in the dielectric continuum solvent,\n\\[\\Delta G_\\text{solv} = (E_\\text{soln} + G_\\text{nes}) - E_\\text{gas}\\]\nwhere \\(E_\\text{soln}\\) and $ E_$ are the electronic energies of the solute with and without the continuum solvent field and \\(G_\\text{nes}\\) refers to the sum of any nonelectrostatic contributions to the solvation free energy, such as cavitation and dispersion-repulsion interactions. The main point here is that the solvation free energy is obtained from the difference in electronic energies, not a difference in free energies computed in solution- and gas-phases. This is because of the way the continuum models are parametrized and from the fact that the ideal gas models used to compute the free energies only apply in the gas-phase.\nSo, in order to calculate all the components of our thermodynamic cycle we will: 1. Optimize all the species in the upper-rung of the thermodynamic cycle in gas-phase 2. Compute the total energy and all the free energy terms for these species 3. Optimize all the species in the lower-rung of the thermodynamic cycle with the dielectric continuum solvent model\nWe will then compute the free energy for the reaction based on the gas-phase free energies,\n\\[\\Delta G_\\text{bind, g} = \\sum_i \\nu_i G_\\text{i} - \\sum_a \\nu_a G_a\\]\nwhere \\(i\\) and \\(a\\) correspond to product and reactant species, respectively.\nThe solvation energies are then computed using the electronic energies of the species (not including the enthalpy and entropy-terms used in gas-phase calculation). Often, especially for ionic systems, it is preferred to employ a hybrid solvation model, that is to use both explicit and implicit solvent together. This will signicantly improve the accuracy of our solvation energies.\n\n\n\ntitle\n\n\n\n\nExample calculations using PySCF\nLet’s proceed step-by-step using the PySCF and calculate the solvation energy of copper(II)-ion.\n\nImplicit solvent only\nWe’ll define a simple system consisting of the copper-ion in gas-phase and with implicit solvent only.\n\nfrom pyscf import gto, scf, solvent\n\nmol = gto.Mole()\nmol.atom = 'Cu 0 0 0' \nmol.basis = 'def2-svp' \nmol.charge = 2 \nmol.spin = 1\nmol.build()\n\nmf = scf.UKS(mol)\nmf.xc = 'PBE'\nmf.verbose = 0\nmf.run()\n\n# Add implicit solvent\nmf_solvent = solvent.ddCOSMO(mf)\nmf_solvent.with_solvent.eps = 78.39\nmf_solvent.verbose = 0\nmf_solvent.mo_coeff = mf.mo_coeff\nmf_solvent.run()\n\nprint(f\"Total energy in vacuum: {mf.e_tot: .3f} Eh\")\nprint(f\"Total energy in solvent: {mf_solvent.e_tot: .3f} Eh\")\nprint(f\"Solvation free energy: {(mf_solvent.e_tot - mf.e_tot) * 627.5: .3f} kcal/mol\")\n\nTotal energy in vacuum: -1638.931 Eh\nTotal energy in solvent: -1639.475 Eh\nSolvation free energy: -341.570 kcal/mol\n\n\nThe evaluated solvation energy is quite off the mark, considering that the solvation free is estimated to be about -500 kcal/mol (see https://pubs.acs.org/doi/abs/10.1021/acs.jpca.8b06674).\n\n\nHybrid solvation model\nLet’s embed the copper-ion in a solvation shell formed by water molecules and compute the solvation free energy using the thermodynamic cycle discussed above. This is considerably more involved calculation, since we need to compute the Hessian for the gas-phase species.\nHow do we obtain a structure for the solvation shell? This is a good question. Some might be found from the literature, at least for a similar ion. An alternative approach would be to construct one using a low-level model, e.g. density functional tight binding (DFTB) or similar approach. Here, we will adopt an artificially small cluster with four water molecules. There are however larger solvation structures available from the literature (see e.g. https://pubs.acs.org/doi/10.1021/jp804373p).\n\n\nWater cluster structures\nConsider the two water clusters, with and without a copper-ion, preoptimized using PBE and def2-SVP basis set.\nWe will use these clusters to calculate the gas-phase reaction for the binding of the copper-ion with the solvent molecules. We will also use them to calculate the solvation energy for each cluster. The solvated structure should be optimized, but we will neglect this step here since the calculations, albeit with a small basis set, are already quite cumbersome to run on a laptop/desktop.\n\n\n\ntitle\n\n\n\nfrom pyscf.hessian import thermo\n\n# First, compute copper with the 'solvation cluster'\ncoords = '''\n  Cu  0.00770670007957     -0.01075714803486      0.00684346198661\n  O   1.62629858055350      0.59958796551543     -0.96137824606612\n  H   2.38274772811028      0.05517739425091     -1.27095980213712\n  H   1.63486316135084      1.44991263891621     -1.45427267152436\n  O   -0.08513692098164      1.85563301721865      0.70772230089222\n  H   -0.92091296008101      2.27895856953297      1.00777628197219\n  H   0.63973505010478      2.22714364462856      1.26089012590088\n  O   -0.16307318602147     -1.71475219698127     -1.01669913168950\n  H   0.12868744546777     -1.79759035478399     -1.95233645651861\n  H   -0.00508273379089     -2.58677644056665     -0.58749562922375\n  O   -1.47600452153588     -0.67910102216609      1.13003992630434\n  H   -1.68821978806555     -0.41530609819069      2.05174002936686\n  H   -2.28219008519028     -1.07343020933916      0.72891087073636\n'''\n\ntemperature = 298.15  # Standard temperature in K\npressure = 101325  # Standard pressure in Pa\n\nmol_cluster = gto.M(atom=coords, basis='def2-svp', charge=2, spin=1)\nmf_cluster = scf.UKS(mol_cluster)\nmf_cluster.xc = 'PBE'\nmf_cluster.verbose = 0\nmf_cluster.run()\n\nhessian = mf_cluster.Hessian().kernel()\n# Frequency analysis\nfreq_cluster = thermo.harmonic_analysis(mf_cluster.mol, hessian)\nthermo_cluster = thermo.thermo(mf_cluster, freq_cluster['freq_au'], temperature, pressure)\n\n\n# Second, compute just the 'solvation cluster'\ncoords = '''\n  O   1.59569157211650      0.52671408407826      0.55875125592345\n  H   1.22572269113165      0.40613430342645     -0.37753454653395\n  H   1.65562959781890     -0.39080578496591      0.88476545887585\n  O   -1.80219894908837      1.36621874756072     -0.76471975545405\n  H   -1.46831586426498      1.39336098953180      0.19232417112175\n  H   -1.87403013018684      2.30887164269416     -1.00164540312905\n  O   0.36899920133373      0.20801276276540     -1.77155683816915\n  H   0.69644080847323      0.81096729919872     -2.46358962449703\n  O   -0.64414560352736      1.46201234097889      1.62413639246884\n  H   -0.47158383660499      2.36988722572480      1.93233540537252\n  H   0.25833209306559      1.14749559670315      1.28200372985479\n  H   -0.51999897026706      0.60946637230353     -1.49946089583396\n'''\n\nmol_solvent = gto.M(atom=coords, basis='def2-svp', charge=0, spin=0)\nmf_solvent = scf.UKS(mol_solvent)\nmf_solvent.xc = 'PBE'\nmf_solvent.verbose = 0\nmf_solvent.run()\n\nhessian = mf_solvent.Hessian().kernel()\n# Frequency analysis\nfreq_solvent = thermo.harmonic_analysis(mf_solvent.mol, hessian)\nthermo_solvent = thermo.thermo(mf_solvent, freq_solvent['freq_au'], temperature, pressure)\n\n\n# Third, compute just the copper ion separately\ncoords = '''\n  Cu   0.0      0.0      0.0\n'''\n\nmol_cu = gto.M(atom=coords, basis='def2-svp', charge=2, spin=1)\nmf_cu = scf.UKS(mol_cu)\nmf_cu.xc = 'PBE'\nmf_cu.verbose = 0\nmf_cu.run()\n\n&lt;pyscf.dft.uks.UKS at 0x7fa11f128610&gt;\n\n\nFor the gas-phase copper-ion, we need to compute the translational entropy using the Sackur-Tetrode equation,\n\\[S_\\text{trans} = R \\left( \\ln \\left( \\frac{kT}{P} \\left( \\frac{2\\pi mkT}{h^2} \\right)^{\\frac{3}{2}} \\right) + \\frac{5}{2} \\right)\n\\]\n\nfrom pyscf.data import nist\nimport numpy as np\n\n# Conversion factors from PySCF's NIST database\nkB = nist.BOLTZMANN\nh = nist.PLANCK\n# Gas constant in units of Hartree\nR_Eh = kB*nist.AVOGADRO / (nist.HARTREE2J * nist.AVOGADRO)\n# Mass of the atom\nmass_tot = np.sum(mol_cu.atom_mass_list()) * nist.ATOMIC_MASS\n# Sackur-Tetrode equation for the translational entropy\nS_trans = R_Eh * (np.log((2.0 * np.pi * mass * kB * temperature / h**2)**1.5 * kB * temperature / pressure) + 5/2)\nthermo_cu = mf_cu.e_tot - temperature * S_trans\n\n\ndGbind_gas = (thermo_cluster['G_tot'][0] - thermo_solvent['G_tot'][0] - thermo_cu) * 627.5\nprint(f\"Gibbs free energy for the gas-phase reaction: {dGbind_gas: .3f} kcal/mol\")\n\nGibbs free energy for the gas-phase reaction: -299.312 kcal/mol\n\n\nThe solvation energies are computed from the electronic energies between the gas-phase and solvated molecules.\n\n# Add implicit solvent to cluster and solvent\nmf_cluster_solvation = solvent.ddCOSMO(mf_cluster)\nmf_cluster_solvation.with_solvent.eps = 78.39\nmf_cluster_solvation.verbose = 0\nmf_cluster_solvation.mo_coeff = mf_cluster.mo_coeff\nmf_cluster_solvation.run()\n\nmf_solvent_solvation = solvent.ddCOSMO(mf_solvent)\nmf_solvent_solvation.with_solvent.eps = 78.39\nmf_solvent_solvation.verbose = 0\nmf_solvent_solvation.mo_coeff = mf_solvent.mo_coeff\nmf_solvent_solvation.run()\n\ndGsolv_cluster = (mf_cluster_solvation.e_tot - mf_cluster.e_tot) * 627.5\ndGsolv_solvent = (mf_solvent_solvation.e_tot - mf_solvent.e_tot) * 627.5\nprint(f\"Solvations free energies for the cluster: {dGsolv_cluster: .3f} kcal/mol\")\nprint(f\"Solvations free energies for the solvent: {dGsolv_solvent: .3f} kcal/mol\")\n\nSolvations free energies for the cluster: -202.658 kcal/mol\nSolvations free energies for the solvent: -8.953 kcal/mol\n\n\nWe also need to account for the change in the standard state when going from gas-phase to the solution phase:\n\nprint(f\"Solvation free energy of copper(II) in water: {dGbind_gas + dGsolv_cluster - dGsolv_solvent: .3f} kcal/mol\")\n\nSolvation free energy of copper(II) in water: -493.016 kcal/mol\n\n\nThis gets very close to the experimental value, although we have performed only very simple calculations with DFT+GGA, no van der Waals correction and only a small basis set. Using more accurate setup and a larger solvation cluster should improve on this. The convergence of the results with respect to the basis set used and the size of the solvation cluster should be checked."
  },
  {
    "objectID": "blog/blog_buildhf_part2.html",
    "href": "blog/blog_buildhf_part2.html",
    "title": "Let’s build Hartree-Fock part 2: orthogonalization procedures",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nThis tutorial covers topics from sections 3.4.2 and 3.4.5 as well as of pages 173-174 from the Modern Quantum Chemistry by Szabo and Ostlund.\n\nFrom the Schrödinger equation to the Hartree-Fock equations\nLet’s have a look at the many-electron Schrödinger equation we are eventually trying to solve:\n\\[\\hat{H}_\\text{elec} = -\\frac{1}{2} \\sum_{i=1}^N \\nabla_i^2-\\sum_{i=1}^N \\sum_{A=1}^M  \\frac{Z_A}{r_{iA}}+ \\sum_{i=1}^N \\sum_{j&gt;i}^N \\frac{1}{r_{ij}}.\\]\nIn the Hartree-Fock theory, we approximate the exact wave function using only a single Slater determinant:\n\\[\n\\Psi(\\mathbf{r}_1, \\mathbf{r}_2, \\ldots, \\mathbf{r}_N) = \\frac{1}{\\sqrt{N!}}\n\\begin{vmatrix}\n\\phi_1(\\mathbf{r}_1) & \\phi_2(\\mathbf{r}_1) & \\cdots & \\phi_N(\\mathbf{r}_1) \\\\\n\\phi_1(\\mathbf{r}_2) & \\phi_2(\\mathbf{r}_2) & \\cdots & \\phi_N(\\mathbf{r}_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\phi_1(\\mathbf{r}_N) & \\phi_2(\\mathbf{r}_N) & \\cdots & \\phi_N(\\mathbf{r}_N)\n\\end{vmatrix}\n\\]\nA Slater determinant consists of one-electron wave functions \\(\\phi_i\\), called orbitals. It is constructed in a product form, meaning that the orbitals are independent of each other and interact only in an average manner via the Coulomb term. The determinant structure assures the product wave function is correctly antisymmetric.\nWhen we minimize the expectation value of the Hamiltonian using a single Slater determinant, we obtain a set of equations for each orbital, called the Hartree-Fock equations:\n\\[\\hat{F} \\phi_i(\\mathbf{r}) = \\epsilon_i \\phi_i(\\mathbf{r})\\]\nwhere \\(\\hat{F}\\) is the Fock operator,\n\\[\\hat{F}(\\mathbf{r}) = -\\frac{1}{2}\\nabla^2 + V_{\\text{ext}}(\\mathbf{r}) + \\int \\frac{\\rho(\\mathbf{r'})}{|\\mathbf{r} - \\mathbf{r'}|}d\\mathbf{r'} - \\sum_{j} \\left(\\hat{J}_j + \\hat{K}_j\\right)\\]\n\nThe first term is the kinetic energy of the orbital \\(V_{\\text{ext}}\\) is the potential energy due to external fields, such as the attraction from nuclei.\nThe third term is the classical Coulomb repulsion integral\n$_j $ and $ _j$ are the Coulomb and exchange operators, respectively, accounting for electron-electron repulsion and the antisymmetry of the wave function.\n\n\n\nFrom the Hartree-Fock equations to the Roothan-Hall equations\nThe next question is: how do we actually solve the Hartree-Fock equations? The Couloumb and exchange operators are especially tricky, since they include the electron-electron interaction and depend on all the other orbitals. A common approach is to expand the orbtails on a finite basis, resulting in the so-called Roothan-Hall equations:\n\\[\\mathbf{F}\\mathbf{C} = \\mathbf{S}\\mathbf{C}\\mathbf{\\epsilon}\\]\nhere * \\(\\mathbf{C}\\) is the matrix of molecular orbital coefficients. Each column corresponds to a molecular orbital, expressed as a linear combination of the basis set functions * \\(\\mathbf{S}\\) is the overlap matrix, each element \\(S_{\\mu\\nu}\\) contains the overlap integrals between the basis functions \\(\\mu\\) and \\(\\nu\\) * \\(\\mathbf{\\epsilon}\\) is a diagonal matrix of orbital energies, each element corresponding to the energy of a molecular orbital * \\(\\mathbf{F}\\) is the so-called Fock matrix,\n\\[F_{\\mu\\nu} = H_{\\mu\\nu}^{\\text{core}} + \\sum_{\\lambda\\sigma}D_{\\lambda\\sigma}\\left( 2(\\mu\\nu|\\lambda\\sigma) - (\\mu\\lambda|\\nu\\sigma) \\right)\\]\nwhere \\(\\mathbf{D}\\) is the so-called density matrix, describing the population of each orbital in the system.\nThe core Hamiltonian consists of the one-electron components, i.e. the kinetic and electron-nucleus interaction terms. The latter term is the Coulomb and exchange interaction between the electrons.\nThe above equation is close to an eigenvalue problem: the only difference is the overlap matrix, due to the basis functions not being orthogonal. If there were no overlap between the basis functions, the overlap matrix would reduce to just an identity matrix. So, to get rid of this overlap we need to orthogonalize the basis set. There are a couple of options to achieve this.\n\nAn example case\nLet’s look at a simple case study: two non-orthogonal vectors \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) on a plane. These two vectors are not parallel and, hence, span the whole plane: we can write any vector on the plane as a linear combination of these two vectors. Suppose that we wish to find an orthogonal basis based on these two vectors.\n\n\nThe Gram-Schmidt procedure\nWe can start by choosing vector \\(\\vec{v}_1\\) as a basis vector. If we then remove the projection of \\(\\vec{v}_2\\) on \\(\\vec{v}_1\\) from \\(\\vec{v}_2\\), we are left with two orthogonal vectors:\n\\[\\vec{v}_2^\\text{new} = \\vec{v}_2-\\text{proj}_{\\vec{v}_1}{\\left(\\vec{v}_2\\right)}\\]\n\n\nThe symmetric procedure\nCompute an overlap matrix \\(S\\) with \\(S_{ij} = \\langle \\mathbf{v}_i |  \\mathbf{v}_j \\rangle\\), symmetric orthogonalization proceeds as follows: 1. Compute the eigenvalue decomposition of \\(S: S = UsU^T\\), where \\(s\\) is a diagonal matrix of eigenvalues, and \\(U\\) contains the corresponding eigenvectors. 2. Construct a transformation matrix \\(X = S^{-1/2} = Us^{-1/2}U^T\\) and apply it to the non-orthogonal vectors.\n\n\nThe canonical procedure\nIn canonical orthogonalization, transformation matrix is \\(X = Us^{-1/2}\\)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef gram_schmidt(vectors):\n    basis = []\n    for v in vectors:\n        w = v - sum(np.dot(v, b)*b for b in basis)\n        if (w &gt; 1e-10).any():  # to avoid zero vectors\n            basis.append(w/np.linalg.norm(w))\n    return np.array(basis)\n\ndef symmetric_orthogonalization(vectors):\n    S = np.array([[np.dot(vi, vj) for vj in vectors] for vi in vectors])\n    eigvals, eigvecs = np.linalg.eigh(S)\n    eigvals_matrix = np.diag(1.0 / np.sqrt(eigvals))\n    transformation_matrix = eigvecs @ eigvals_matrix @ eigvecs.T\n    return np.dot(transformation_matrix.T, vectors) # Apply transformation matrix\n\ndef canonical_orthogonalization(vectors):\n    S = np.array([[np.dot(vi, vj) for vj in vectors] for vi in vectors])\n    eigvals, eigvecs = np.linalg.eigh(S)\n    transformation_matrix = eigvecs @ np.diag(1.0 / np.sqrt(eigvals))\n    return np.dot(transformation_matrix.T, vectors) # Apply transformation matrix\n\ndef plot_vectors(arr1, arr2, arr3, arr4, ax, title):\n    ax.arrow(0, 0, arr3[0], arr3[1], head_width=0.05, head_length=0.07, fc='black', ec='black')\n    ax.arrow(0, 0, arr4[0], arr4[1], head_width=0.05, head_length=0.07, fc='black', ec='black')\n    ax.arrow(0, 0, arr1[0], arr1[1], head_width=0.05, head_length=0.07, fc='r', ec='r')\n    ax.arrow(0, 0, arr2[0], arr2[1], head_width=0.05, head_length=0.07, fc='r', ec='r')\n    ax.set_title(title)\n    ax.set_aspect('equal')\n    ax.set_xlim(-0.7,1.5)\n    ax.set_ylim(-0.7,1.5)\n\nfig, axs = plt.subplots(1, 3)\nfig.tight_layout(pad=1.0)\nfig.set_figheight(12)\nfig.set_figwidth(12)\n# Two non-orthogonal vectors\narr1 = np.array([1.0, 0.0, 0.0])\narr2 = np.array([1.0, 1.0, 0.0])\n# Apply different orthogonalization procedures\narr3, arr4 = gram_schmidt([arr1, arr2]) * 1.2\narr5, arr6 = symmetric_orthogonalization([arr1, arr2])\narr7, arr8 = canonical_orthogonalization([arr1, arr2])\nplot_vectors(arr1, arr2, arr3, arr4, axs[0], title='Gram-Schmidt')\nplot_vectors(arr1, arr2, arr5, arr6, axs[1], title='Symmetric')\nplot_vectors(arr1, arr2, arr7, arr8, axs[2], title='Canonical')\nplt.show()\n\n\n\n\n\n\n\n\nAbove we see the three different approaches in action. In the Gram-Schmidt procedure, we choose one of the vectors to be the first basis vector. In higher dimensional case, we would proceed constructing the basis vector by vector, removing all the overlap from the set of vectors. In the symmetric procedure, the new orthogonal vectors most closely resemble the original vectors. The angle between the two vectors is opened up to 90 degrees. In the canonical procedure, one of the new vectors bisects the angle between the original vectors and the second vector is orthogonal to the first."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Quantum of Science",
    "section": "About me",
    "text": "About me\nI am a university researcher at the University of Jyväskylä, working on heterogeneous catalysis, electrochemistry and surface chemistry. I have a Master’s degree in Chemical Technology and a PhD in Computational Chemistry from Aalto University as well as Master’s degree from Mathematics from the University of Helsinki and a strong background in chemistry, physics and programming. I have a lot of experience in interdisciplinary collaboration, communicating my own work to colleagues and helping them to apply my results in their work. I also have a long history of teaching to a wide range of audiences, from freshmen to graduate students. My publication list can be found here.\nBelow I have collected some notes and tutorials. For those who come after."
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Quantum of Science",
    "section": "Blog posts",
    "text": "Blog posts"
  },
  {
    "objectID": "blog/blog_neb.html",
    "href": "blog/blog_neb.html",
    "title": "Let’s build! Nudged elastic band method from first principles",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nThe nudged elastic band method\nNudged elastic band method, or NEB for short, is an algorithm to find a minimum energy pathway between two point on a multi-dimensional potential energy surface (PES). The idea is to span the pathway on the PES between the initial and final points into increments, each point (or image) on the pathway connected to one another via a spring force. The energy of the overall path (or band, since the path is essentially elastic due to the spring force) is then minimized. After optimization, the band should lie along the minimum energy pathway (MEP).\nIn this tutorial, we’ll construct and apply an NEB procedure to a simple two dimensional potential energy surface, defined as\n\\[V(x,y) = \\exp\\left[-\\cos(2\\pi x) - \\cos(\\pi (y-x))\\right] - \\exp\\left[-\\cos(2\\pi y)\\right].\\]\nWe’ll use the climbing image variant of NEB, where we’ll find the saddle point along the minimum energy path that corresponds to the highest peak along the path.\n\n\nThe NEB procedure\nIn NEB, we first define two close lying minima on the PES and construct an initial guess for the MEP. The guess is often done via interpolation. The idea in NEB is to perform energy minization on each image of the reaction path, but remove the force component along the NEB path with a spring force. Why? If we were just to minimize the energy without the spring force, all the images would slide down the potential energy surface to the nearest minimum. The spring force binds the neighbouring images together and prevents them from moving too far apart. The spring force acts only along the path defined by the images. Orthogonal to the path we use the actual force acting on the images to minimize the energy. This way the band will aling itself with the shape of the potential energy surface.\nThere are several way to define the path of the NEB band. Here, we’ll define it as the difference between the given image \\(i\\) and the neighbours before and after it depending on what is the relative energies of the images:\n\\[\\hat{\\tau}_i = \\begin{cases}\n\\tau^+_i &= \\mathbf{R}_{i+1}-\\mathbf{R}_{i} \\;,\\;\\;\\text{if} \\;E(\\mathbf{R}_{i+1}) &gt;E(\\mathbf{R}_{i}) &gt; E(\\mathbf{R}_{i-1}), \\\\\n\\tau^-_i &= \\mathbf{R}_{i}-\\mathbf{R}_{i-1} \\;,\\;\\;\\text{if} \\;E(\\mathbf{R}_{i+1}) &lt;E(\\mathbf{R}_{i}) &lt; E(\\mathbf{R}_{i-1}). \\\\\n\\end{cases}\\]\nIf the potential energy of image \\(i\\) is an extremum, the path is then \\(\\tau = \\mathbf{R}_{i+1}-\\mathbf{R}_{i-1}\\). The pathvector is normalized.\nThe forces acting on each image are divided into two orthogonal components, one parallel to the path and one orthogonal:\n\\[\\mathbf{F}_i = \\mathbf{F}_{i,\\parallel} + \\mathbf{F}_{i,\\perp}.\\]\nThe parallel path is defined as a spring force that will ensure that the images do not all just fall down the PES to the their closest minimum:\n\\[\\mathbf{F}_{i,\\parallel} = k(|\\textbf{R}_{i+1}-\\mathbf{R}_{i}| - |\\mathbf{R}_{i}-\\mathbf{R}_{i-1}|)\\hat{\\tau}_i,\\]\nwhere \\(k\\) is the spring constant for the harmonic force. The orthogonal component is obtained from the the ‘real’ force acting on the system, i.e. the force we get as the gradient of the potential energy:\n\\[\\mathbf{F} = -\\nabla V(\\mathbf{R}_i).\\]\nTo retain only the orthogonal component to the path vector \\(\\tau_i\\), we need to remove the projection of the force parallel to the path:\n\\[\\mathbf{F}_{i,\\perp} = -\\nabla V(\\mathbf{R}_i) + \\left[\\nabla V(\\mathbf{R}_i)\\cdot \\hat{\\tau}_i \\right] \\hat{\\tau}_i.\\]\nIn the climbing image (CI) NEB, the image with the highest energy is not subjest to the spring force. Instead, it will try to climb up the potential energy surface instead of going down. This means that the component parallel to the NEB path will be reversed:\n\\[\\mathbf{F}_{i, \\text{max}} = -\\nabla V(\\mathbf{R}_{i, \\text{max}}) + \\left[2\\nabla V(\\mathbf{R}_{i, \\text{max}})\\cdot \\hat{\\tau}_{i, \\text{max}}\\right] \\hat{\\tau}_{i, \\text{max}}.\\]\n\n\nBuilding NEB for a 2D potential\nWe’ll use the steepest descent method with a fixed step size to minimize the energy of the band. In steepest descent, we follow the direction of the steepest descent (i.e. the force vector) and update the coordinates of each image consecutively using a fixed step size.\nIn addition to Numpy, we’ll use Sympy library for symbolic calculation to do the partial derivatives of the potential function. Let’s begin by defining the potential energy surface for our system.\n\nimport sympy as sp\nfrom IPython.display import display\n\n# Define symbolic variables\nx, y = sp.symbols('x y')\n# Define the function V(x, y)\nV_sp = sp.exp(-sp.cos(2*sp.pi*x) - sp.cos(sp.pi*(y-x))) - sp.exp(-sp.cos(sp.pi*y))\n\nNext, we’ll compute the partial derivatives of the potential energy function and convert the experssions into a separate functions using the lambdify-function. We can then readily use these functions in our NEB procedure. We can use the latex-function in Sympy to print out Latex expressions for our partial derivatives. A similar python or print_python function would give an expression in Python that could then be defined separately.\n\n# Compute the partial derivatives using Sympy\ndVx_sp = sp.diff(V_sp, x)\ndVy_sp = sp.diff(V_sp, y)\n# Lambdify the expressions\nV = sp.lambdify((x, y), V_sp, 'numpy')\ndVx = sp.lambdify((x, y), dVx_sp, 'numpy')\ndVy = sp.lambdify((x, y), dVy_sp, 'numpy')\n\nprint(sp.latex(dVx_sp))\nprint(sp.latex(dVy_sp))\n\n\\left(2 \\pi \\sin{\\left(2 \\pi x \\right)} - \\pi \\sin{\\left(\\pi \\left(- x + y\\right) \\right)}\\right) e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}}\n\\pi e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}} \\sin{\\left(\\pi \\left(- x + y\\right) \\right)} - \\pi e^{- \\cos{\\left(\\pi y \\right)}} \\sin{\\left(\\pi y \\right)}\n\n\nThe partial derivatives are then:\n\\[\\frac{\\partial V}{\\partial x} = \\left(2 \\pi \\sin{\\left(2 \\pi x \\right)} - \\pi \\sin{\\left(\\pi \\left(- x + y\\right) \\right)}\\right) e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}}\\]\n\\[\\frac{\\partial V}{\\partial y} = \\pi e^{- \\cos{\\left(2 \\pi x \\right)} - \\cos{\\left(\\pi \\left(- x + y\\right) \\right)}} \\sin{\\left(\\pi \\left(- x + y\\right) \\right)} - \\pi e^{- \\cos{\\left(\\pi y \\right)}} \\sin{\\left(\\pi y \\right)}\\]\nFinally, let’s plot the potential energy surface as a contour plot. The points \\([0,1]\\) and \\([1,1]\\) are two minima on the PES. We’ll be using these as the initial and final points, respectively. We’ll mark these two points down using a red dot.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(-0.6, 1.4, 40)\ny = np.linspace(0.5, 1.4, 40)\nX, Y = np.meshgrid(x, y)\nZ = V(X,Y)\n\nfig, ax = plt.subplots()\nax.contour(X, Y, Z, 30, colors='black', linestyles='solid')\nax.scatter([0,1], [1,1], color='r')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Potential energy surface')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll begin by defining an initial guess for the minimum energy path. A good initial guess is crucial for the accuracy of the minimum energy path, since we really only relax the initial guess and do not explore the potential energy surface beyond our guess. A linear interpolation between the initial and final states is a common approximation.\n\n# Initial state (0,1)\n# Final state (1,1)\n\nnimg = 16 # Number of images\ninit = np.array([0,1]) # Initial state on the PES\nfinal = np.array([1,1]) # Final state on the PES\n\n# Interpolate path, save full path to images\nimages = [init]\nfor img in range(1,nimg+1):\n    images.append(init - (init-final) / (nimg+1) * (img))\nimages.append(final)\nimages = np.array(images)\n\nNext, we’ll initialize the forces and energies of the system. The forces will be used in our steepest descent optimization scheme and the energies are used for defining the path vector and for choosing the climbing image, each cycle.\n\n# List of forces on all images (init and final states will remain stationary)\nforces = np.zeros((len(images),2))\nenergies = np.zeros((len(images),))\n# Compute energies for the initial and final states\nenergies[0] = V(images[0,0], images[0,1])\nenergies[-1] = V(images[-1,0], images[-1,1])\n\nNext, we’ll define the NEB procedure. First, we’ll evaluate the energy of each image and then loop over all the images in our band and, 1. Construct the path vector 2. Normalize the path vector 3. Compute the real force 4. Compute the spring force 5. Apply the forces in the optimization\n\n# NEB parameters\nstep_size = 0.01 # Step size for optimisation1\nk = 0.005 # Spring constant for the harmonic force\n\ninitial_images = images.copy() # Copy the initial guess for the path\nreal_force = np.zeros((2,))\npath_history = [images.copy()] # Record the pathway from beginning to end, for plotting purposes\nfor step in range(70): # Run the optimisation for some fixed amount of steps\n    for i in range(1, len(images)-1):\n        x, y = images[i]\n        energies[i] = V(x,y)\n    for i in range(1, len(images)-1):\n        x, y = images[i]\n\n        # Construct the path vector\n        if energies[i] &gt; energies[i-1] and energies[i] &lt; energies[i+1]:\n            path = images[i+1] -  images[i]\n        elif energies[i] &lt; energies[i-1] and energies[i] &gt; energies[i+1]:\n            path = images[i] -  images[i-1]\n        else:\n            path = images[i+1] - images[i-1]\n        path /= np.linalg.norm(path) # Normalize the path vector\n\n        real_force = -np.array([dVx(x,y), dVy(x,y)]) # Compute the real force\n        spring_force = k * (np.linalg.norm(images[i+1]-images[i]) - np.linalg.norm(images[i]-images[i-1])) * path # Compute the spring force\n        # Steepest descent optimization\n        if energies[i] == np.max(energies): # If the image has the highest potential energy, use the climbing image procedure\n            forces[i] = real_force - 2 * np.dot(real_force, path) * path\n        else: # Else, use only the orthogonal component of the real force and the spring force\n            forces[i] = real_force - np.dot(real_force, path) * path + spring_force\n        \n    if np.max(np.sum(forces**2, axis=1)**0.5) &lt; 0.05:\n        break\n\n    images[1:-1] += step_size * forces[1:-1] / np.linalg.norm(forces[1:-1]) # Steepest descent optimisation\n    path_history.append(images.copy())\n\nxopt, yopt = images[energies == np.max(energies)][0]\neopt = energies[energies == np.max(energies)][0]\nprint(f\"NEB saddlepoint at ({xopt: .3f}, {yopt: .3f}) with energy {eopt: 0.7f}\")\nprint(f\"Exact saddlepoint at (0.427, 0.777) with energy {V(0.427, 0.777): 0.7f}\")\n\nNEB saddlepoint at ( 0.427,  0.775) with energy -0.5910029\nExact saddlepoint at (0.427, 0.777) with energy -0.5910308\n\n\nWe find a saddle point very close to an exact solution. Let’s plot our initial and the optimized NEB paths on the potential energy surface.\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-0.4, 1.2, 40)\ny = np.linspace(0.7, 1.2, 40)\nX, Y = np.meshgrid(x, y)\nZ = V(X,Y)\n\nfig, ax = plt.subplots()\nax.contour(X, Y, Z, 40, colors='black', linestyles='solid')\nax.scatter(initial_images[:,0], initial_images[:,1], color='r')\nax.scatter(images[:,0], images[:,1], cmap='binary', color='black')\nax.set_xlabel('x')\nax.set_ylabel('y')\n#fig.savefig('plot.png')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s animate the path to see how the band relaxes onto the minimum energy path.\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n# Ensure that animations are rendered inline\n%matplotlib inline\n\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z, 40, colors='black', linestyles='solid')\nscatter_initial = ax.scatter([], [], color='r')\nscatter_images = ax.scatter([], [], color='black')\nplt.close(fig)\n\ndef data_from_history(iteration):\n    if iteration &lt; len(path_history):\n        return path_history[0], path_history[iteration]\n    else:\n        return path_history[0], path_history[-1]\n\n# Update function for animation\ndef update(frame):\n    initial_images, images = data_from_history(frame)\n    scatter_initial.set_offsets(initial_images)\n    scatter_images.set_offsets(images)\n    return scatter_initial, scatter_images,\n\n# Create animation\nani = FuncAnimation(fig, update, frames=range(80), blit=True)\nHTML(ani.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe initial movement is largest where the gradient is largest (the points on top of the high energy ridge). The highest energy point slides down towards the saddle point. After the gradient acting on the highest energy images decreases, all the point start to relax. The spacing between the points is not uniform. Where the potential energy is relatively flat, the points have larger freedom of movement and align themselves with the contours of the potential energy surface with larger spacing between the points."
  },
  {
    "objectID": "blog/blog_tddft_c6.html",
    "href": "blog/blog_tddft_c6.html",
    "title": "How to: Dispersion coefficients from TDDFT",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\n\nDispersion and \\(C_6\\) coefficients\nDispersion interaction is a weak attractive interaction that binds even neutral, noble gas atoms to each other. Dispersion interaction sometimes mistakenly called the van der Waals (vdW) interaction, which is not correct. VdW-interactions are a combination of intermolecular interactions, where permanent molecular dipoles induce and interact with other molecular dipoles. Despite being the weakest form of van der Waals forces, dispersion interactions significantly influence the structure, stability, and properties of molecular systems, including biological molecules like proteins and DNA.\nDispersion interaction is caused by fluctuations in the electron densities in an atom or a molecule. The fluctuations cause intantaneous electric fields that are felt by ther nearby atoms or molecules, which respond to the electric field by adjusting their own electron densities accordingly. These instantaneous dipole moments then attract each other, resulting in the dispersion interaction.\nWhile other van der Waals force are well described by the computational chemistry workhorse, the density functional theory (DFT), the dispersion interaction is not. Traditional DFT methods struggle to accurately predict dispersion interaction, because the electron-electron interaction is only treated approximately, without directly accounting for the correlated motion of electrons. Instead, the dispersion is usually included in the DFT calculations as a separate correction. Using the Rayleigh–Schrödinger perturbation theory, one can show that the potential energy between two weakly interacting systems is proportional to the inverse sixth power of the distance between them, leading to the well-known\n\\[V(r) \\propto -\\frac{C_6}{r^6}\\]\nwhere \\(C_6\\) is the dispersion coefficient and \\(r\\) is the intermolecular distance.\nHere, we will have a look at how the dispersion coefficient \\(C_6\\) can be computed using time-dependent density functional theory (TD-DFT) by PySCF. We will also explore the basis set dependence of the results and how the commonly employed Tamm-Dancoff approximation (TDA) affects the results.\n\n\nOscillator strengths\nOscillator strength describes the probability for transition between quantum states when a photon is absorbed (or emitted),\n\\[f_n = \\frac{2}{3} \\omega_{n0}^A\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\]\nwhere \\(f_n\\) is the oscillator strength for a transition from the ground state to the \\(n\\)th excited state, \\(\\omega_n\\) is the corresponding transition energy and \\(\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\) is the squared magnitude for the transition dipole moment vector of the excitation.\nIn fact, one can use Rayleigh–Schrödinger perturbation theory to derive an expression for the dispersion interaction between two systems \\(A\\) and \\(B\\) separated by a large distance \\(R\\) with no overlap:\n\\[\nU_\\text{disp} = -\\frac{C_6}{R^6}\n\\]\n\\[\nC^{AB}_6 = \\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{\\left|\\left&lt;{\\phi_0^A|\\mathbf{r}|\\phi_n^A}\\right&gt;\\right|^2\\left|\\left&lt;{\\phi_0^B|\\mathbf{r}|\\phi_m^B}\\right&gt;\\right|^2}{\\omega_{n0}^A + \\omega_{m0}^B} = \\frac{3}{2}\\sum_{n\\neq0}\\sum_{m\\neq0}\\frac{f_{n0}^A f_{m0}^B}{\\omega_{n0}^A\\omega_{m0}^B(\\omega_{n0}^A + \\omega_{m0}^B)},\n\\]\nwhere \\(\\omega_{m0} = E_m-E_0\\).\nHere we will use the last sum to compute the dispersion coefficient \\(C_6\\) for a methane molecule.\n\n\nDispersion coefficients with TDDFT from PySCF\nWe will use PySCF to compute the excitation spectra and oscillator strenghts using time-dependent density functional theory (TD-DFT). First we’ll compute the ground state energy of a methane molecule using \\(\\omega\\)B97X exchange–correlation functional and a diffuse triple-\\(\\zeta\\) basis set def2-TZVPD.\n\nfrom pyscf import gto, scf, tdscf, dft, tddft\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define molecule and basis set\nmol = gto.Mole()\nmol.atom = 'H 0.5349075 0.1628792 0.9466881; \\\n            H 0.2074921 0.8335188 -0.6864069; \\\n            H 0.3383696 -0.9421607 -0.4547893; \\\n            H -1.0808570 -0.0543245 0.1943354; \\\n            C -0.0000122 -0.0000128 -0.0000272'\nmol.basis = 'def2-tzvpd'\nmol.build()\n\n# Run DFT calculation\nmf = scf.RKS(mol)\nmf.xc = 'wB97x'\nmf.verbose = 0\nmf.run()\n\n&lt;pyscf.dft.rks.RKS at 0x7a91b3ff8910&gt;\n\n\nNext, we’ll compute the full excitation spectra using the TDDFT module of PySCF. We can define how many excitations are included by the variable. The maximum is determined by the dimension of our basis set.\n\n# Run TDDFT calculation\ntd = tddft.TDDFT(mf)\ntd.nstates = 450  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\n&lt;pyscf.tdscf.rks.TDDFT at 0x7f38c00a4210&gt;\n\n\nLet’s look at how the Thomas–Reiche–Kuhn sum rule is reproduced by our spectra. The TRK sum rule states that the sum of the oscillator strenghts should equal the number of particles in the system, i.e. \n\\[\\sum_n f_n = N.\\]\n\n# Extract excitation energies (in a.u.) and oscillator strengths\nexcitation_energies = np.array(td.e)\noscillator_strengths = np.array(td.oscillator_strength(gauge='length', order=0))\n\nprint(f'Thomas-Reiche-Kuhn sum rule: {oscillator_strengths.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nThomas-Reiche-Kuhn sum rule:  8.847\nNumber of electrons: 10\n\n\nThe sum rule and the number of electrons done exactly match. We’ll come back to this a bit later. However, using the sum for the dispersion coefficients, presented above, we can compute the dispersion coefficient for the methane molecule.\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies[n]\n    for m in range(M):\n        wm = excitation_energies[m]\n        C6 += oscillator_strengths[n]*oscillator_strengths[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nDispersion coefficient:  126.347\n\n\nThe computed dispersion coefficient is excellent good agreement with the experimental value of 129.6 measured using dipole oscillator strength distribution (see https://doi.org/10.1080/00268978000103781).\nAn alternative scheme for computing the dispersion coefficient between system \\(A\\) and \\(B\\) is by the Casimir–Polder equation,\n\\[C_6^{AB} = \\frac{3}{\\pi} \\int_0^\\infty \\alpha^A(i\\omega)\\alpha^B(i\\omega)d\\omega\\]\nwhere \\(\\alpha^{A/B}(\\omega)\\) is the dynamic polarizability of system \\(A/B\\).\nWe can construct the dynamic polarizability from the oscillator strengths, using\n\\[\\alpha^A(\\omega) = \\sum \\frac{f_n^A}{\\omega_n^2-\\omega^n}.\\]\n(For more details, see the discussion by Langhoff and Karplus in The Pade Approximant in Theoretical Physics.)\n\n# Construct dynamic polarizability\nfrequencies = np.linspace(0, 30, 300)\nalpha = np.zeros(frequencies.shape)\n\nfor k in range(len(excitation_energies)):\n    alpha += oscillator_strengths[k] / (excitation_energies[k]**2 + frequencies**2) # The imaginary frequency is accounted for in the sum\n\nC6_alpha = np.trapz(alpha*alpha*3/np.pi, frequencies) # Integrate over the frequencies\nprint(C6_alpha)\n\n126.34656702984459\n\n\n\n\nTamm-Dancoff approximation\nIn TDDFT, the full excitation spectra considered contains both excitations and de-excitations. However, the excitations and de-excitations are somewhat redundant and de-excitations are often ignored to ease the computational burden. This is called the Tamm-Dancoff approximation. TDA is often regarded as a reasonable approximation that, in some instances, provides stabilization to the calculation and even improves the computed excitation spectra.\nLet’s switch the full TDDFT to TDA and see what we get.\n\n# Run TDA calculation\ntd = tddft.TDA(mf)\ntd.nstates = 350  # Number of excitations included\ntd.verbose = 0\ntd.run()\n\nexcitation_energies_tda = np.array(td.e)\noscillator_strengths_tda = np.array(td.oscillator_strength(gauge='length', order=0))\nprint(f'Thomas-Reiche-Kuhn sum rule for TDA: {oscillator_strengths_tda.sum(): .3f}')\nprint(f'Number of electrons: {np.sum(mol.nelec)}')\n\nN = M = len(excitation_energies)\n\nC6 = 0\nfor n in range(N):\n    wn = excitation_energies_tda[n]\n    for m in range(M):\n        wm = excitation_energies_tda[m]\n        C6 += oscillator_strengths_tda[n]*oscillator_strengths_tda[m] / (wn*wm * (wn+wm)) * 3./2.\nprint(f'Dispersion coefficient: {C6: .3f}')\n\nThomas-Reiche-Kuhn sum rule for TDA:  11.374\nNumber of electrons: 10\nDispersion coefficient:  107.405\n\n\nThe dispersion coefficient is well off the mark! Also the TRK sum is larger than the number of electrons in the system. Why is this?\nIf we compare the excitation spectra and the oscillator strengths for full TDDFT and with TDA, we see that while the excitation spectra from both methods agree with each other, the oscillator strenghts do not. This discrepancy results in a poor accuracy for the dispersion coefficient (and dynamic polarizability).\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.tight_layout()\nax1.plot(excitation_energies, excitation_energies_tda, 'o')\nax1.set_title('Excitation energies')\nax1.set_xlabel(\"Full TDDFT\")\nax1.set_ylabel(\"TDA\")\nax2.plot(oscillator_strengths, oscillator_strengths_tda, 'o')\nax2.set_title('Oscillator strengths')\nax2.set_xlabel(\"Full TDDFT\")\nax2.set_ylabel(\"TDA\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBasis set dependence\nA diffuse basis set is in general a must when computing excited states, especially in post-Hartree-Fock theory, must also in TDDFT. If we switch to a smaller basis set (such as def2-TZVP or even def2-SVP), our estimate for the dispersion coefficient becomes significantly much poorer. Interestingly, the TRK sum rule is more closer to \\(N\\)."
  },
  {
    "objectID": "blog/blog_buildhf_part3.html",
    "href": "blog/blog_buildhf_part3.html",
    "title": "Let’s build Hartree-Fock part 3: Roothan-Hall equations",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nThis tutorial covers topics from section 3.4 from the Modern Quantum Chemistry by Szabo and Ostlund. The equations are numbered to correspond to the numbering in the book.\n\nThe Roothan-Hall equations\n\nWhen we expand the orbitals in the Hartree-Fock equations as a linear combination of atomic orbitals (LCAO), we end up with the Roothan-Hall equations (eqn. 3.139):\n\\[\\mathbf{F}\\mathbf{C} = \\mathbf{S}\\mathbf{C}\\mathbf{\\epsilon},\\]\nwhere * \\(\\mathbf{C}\\) is the matrix of molecular orbital coefficients. Each column corresponds to a molecular orbital, expressed as a linear combination of the basis set functions * \\(\\mathbf{S}\\) is the overlap matrix, each element \\(S_{\\mu\\nu}\\) contains the overlap integrals between the basis functions \\(\\mu\\) and \\(\\nu\\) * \\(\\mathbf{\\epsilon}\\) is a diagonal matrix of orbital energies, each element corresponding to the energy of a molecular orbital * \\(\\mathbf{F}\\) is the so-called Fock matrix (eqn. 3.154),\n\\[F_{\\mu\\nu} = H_{\\mu\\nu}^{\\text{core}} + \\sum_{\\lambda\\sigma}D_{\\lambda\\sigma}\\left( 2(\\mu\\nu|\\lambda\\sigma) - (\\mu\\lambda|\\nu\\sigma) \\right),\\]\nwhere \\(\\mathbf{D}\\) is the so-called density matrix, describing the population of each orbital in the system.\nThe Roothan-Hall equations are for the so-called restricted Hartree-Fock system, i.e. we assume an even number of electrons and that all occupied orbitals are populated with a spin up and a spin down electron. The core Hamiltonian consists of the one-electron components, i.e. the kinetic and electron-nucleus interaction terms. The latter term is the two-electron component, consisting of the Coulomb and exchange interaction between the electrons. In order to diagonalize (solve) the Fock matrix, we need to use an orthogonalization scheme discussed in the previous entry. The orthogonalization matrix transforms us from the atomic orbital (AO) basis into the molecular orbital (MO) basis.\nHowever, we cannot solve the Fock matrix directly, since in order to compute the two-electron part of the Fock matrix, we would need to have the orbitals at hand already. Therefore, we need to first get an initial guess of the molecular orbitals. We will then solve the Fock matrix using these guess orbitals and compare whether our new solution matches the guess orbitals. We will then run this solve cycle iteratively until (hopefully) reaching a convergence. This is called the self consistent cycle (SCF).\n\n\nMolecular integrals and orthogonalization\nLet’s have a look at what the different matrices in our Roothan-Hall equation contain. In the LCAO method, a set of atom-centered basis functions are placed on top the atomic nuclei and all the necessary integrals between the different basis functions are computed. This is done well in advance of the SCF. In the different matrices we have - Overlap (eqn. 3.136)\n\\[S_{ij} = \\int \\phi_{i}(\\mathbf{r}) \\phi_{j}(\\mathbf{r}) , d\\mathbf{r}\\]\n\nKinetic energy (eqn. 3.151)\n\n\\[T_{ij} = \\int \\phi_{i}(\\mathbf{r}) \\left( -\\frac{1}{2} \\nabla^2 \\right) \\phi_{j}(\\mathbf{r}) , d\\mathbf{r}\\]\n\nNuclear attraction (eqn. 3.152)\n\n\\[V_{ij} = \\int \\phi_{i}(\\mathbf{r}) \\left( - \\sum_A \\frac{Z_A}{|\\mathbf{r} - \\mathbf{R}A|} \\right) \\phi_{j}(\\mathbf{r}) , d\\mathbf{r}\\]\n\nElectron-electron (eqn. 3.155)\n\n\\[V_{ijkl} = (ij|kl) = \\int \\frac{\\phi_{i}(\\mathbf{r}_1) \\phi{j}(\\mathbf{r}_1) \\phi{k}(\\mathbf{r}_2) \\phi_{l}(\\mathbf{r}_2)}{|\\mathbf{r}_1 - \\mathbf{r}_2|} d\\mathbf{r}_1 d\\mathbf{r}_2\\]\nThe core Hamiltonian is the sum of the kinetic and electron-nucleus interaction terms, \\(H_{\\mu\\nu}^{\\text{core}} = T_{\\mu\\nu} + V_{\\mu\\nu}\\).\nHere, we will have a look at how to construct an initial guess for our molecular orbitals. Implementing these integrals can be very cumbersome, especially for orbitals with angular momentum and instead of implemeting them ourselves, we’’ll use the PySCF quantum chemistry package to gain all the necessary matrices.\nLet’s begin by constructing a PySCF molecule object. We will then extract the molecular integrals for this system using the intor-function.\n\nfrom pyscf import gto, scf, mp\nfrom ase.units import Bohr\nimport numpy as np\n\nm = gto.Mole()\nm.build(atom=f\"H 0 0 0; H {1.4*Bohr} 0 0\", basis=\"3-21g\", spin=0, charge=0)\nTmat = m.intor(\"int1e_kin\") # Integral matrix for kinetic energy, eqn 3.151\nSmat = m.intor(\"int1e_ovlp\") # Integral matrix for overlap integrals. eqn 3.136\nVmat = m.intor(\"int1e_nuc\") # Integral matrix for electron-nuclei integrals, eqn 3.152\nVee = m.intor(\"int2e\") # Integral matrix for electron-electron integrals, eqn 3.155\nnelec = m.nelectron # Number of electrons\nprint(Tmat.shape, Smat.shape, Vmat.shape, Vee.shape)\n\n(4, 4) (4, 4) (4, 4) (4, 4, 4, 4)\n\n\nThe first three matrices are \\(N\\times N\\) matrices, while the \\(\\mathbf{V}_{ee}\\) is a \\(N\\times N\\times N\\times N\\) array. The \\(N\\) is the number of basis functions in our system and depends on the basis set we use and what kind of/how many atoms thereare in our system.\nWe will first use the symmetric orthogonalization method to construct a transformation matrix \\(\\mathbf{X}\\) that allows us to switch from the AO basis to the MO basis.\n\n# Symmetric orthogonalization\ns, U = np.linalg.eigh(Smat)\nX = U / s**0.5 # Eqn 3.169\n\nNext, we’ll construct the Fock matrix from these components. The one-electron part is simple: we’ll just add the kinetic and nucleus parts together. We’ll then use the core Hamiltonian as an initial guess for the molecular orbitals, i.e. assuming that there is no interaction between the electrons. This is quite a crude approximation, but will work on our relatively simple system. Diagonalization will result in MO eigenvalues (energies) and vectors.\n\nHcore_AO = Tmat + Vmat\nHcore_MO = np.dot(X.T, Hcore_AO.dot(X)) # Transform from AO to MO basis\nmo_eigs, mo_vecs = np.linalg.eigh(Hcore_MO) # Diagonalize the Hcore in MO basis\n\nIn order to construct the two-electron part of the Fock matrix, we need a density matrix that contains information on how the electron population is distributed among the AOs. For this, we’lll use the molecular orbitals we obtained from the core Hamiltonian initial guess to construct a density matrix, \\(\\mathbf{D} = \\mathbf{C}^\\dagger \\mathbf{C}\\). First, we’ll transform the eigenvectors (i.e. MO coefficients) back to AO basis, where we have all our AO integrals. We will only consider the occupied molecular orbitals in our density matrix. The eigenvalues we obtain from the Numpy eigh-function are already ordered from the lowest eigenvalue to the highest, so we will just iterate over the first \\(\\frac{N_\\text{elec}}{2}\\) states (this is the aufbau-principle).\n\nCvec = np.dot(X, mo_vecs) # Transform the MO coefficients from MO to AO basis\nDmat = np.zeros((Smat.shape)) # Initialize the density matrix\nfor a in range(nelec//2): # Compute the density matrix in the AO basis. Since we use restricted HF, all spins are paired and only nelec/2 orbitals are considered\n    Dmat += np.einsum('i,j-&gt;ij', Cvec[:, a], Cvec[:, a].T) # Eqn 3.145\n\nThe Numpy function einsum is especially useful and powerful tool here. The density matrix is defined as (eqn 3.145, modified),\n\\[D_{ij} = \\sum\\limits_{a}^{N_\\text{elec}/2} C_{i a} C_{j a}^*\\]\nThis is what our einsum calculates.\nThe full Fock matrix is\n\\[F_{\\mu\\nu} = H_{\\mu\\nu}^{\\text{core}} + \\sum_{\\lambda\\sigma}D_{\\lambda\\sigma}\\left( 2(\\mu\\nu|\\lambda\\sigma) - (\\mu\\lambda|\\nu\\sigma) \\right)\\]\nwhere the latter part is the two-electron component. This can be rewritten as\n\\[\\sum_{\\lambda\\sigma}D_{\\lambda\\sigma}\\left(2(\\mu\\nu|\\lambda\\sigma) - (\\mu\\lambda|\\nu\\sigma) \\right) = 2{J}_{\\mu\\nu} - {K}_{\\mu\\nu}.\\]\nwhere \\(\\mathbf{J}\\) (eqn 2.182) and \\(\\mathbf{K}\\) (eqn 2.184) are called the Coulomb and Exchange parts, respectively:\n\\[{J}_{\\mu\\nu} = D_{\\lambda\\sigma} \\sum\\limits_{\\lambda\\sigma} (\\mu\\nu|\\lambda\\sigma)\\]\n\\[{K}_{\\mu\\nu} = D_{\\lambda\\sigma} \\sum\\limits_{\\lambda\\sigma} (\\mu\\lambda|\\nu\\sigma)\\]\nwith each integral corresponding to an element in the \\(\\mathbf{V}_{ee}\\) matrix,\n\\[V_{\\mu\\nu\\lambda\\sigma} = (\\mu\\nu|\\lambda\\sigma) = \\int \\int \\phi_\\mu(\\mathbf{r}_1) \\phi_\\nu(\\mathbf{r}_1) \\frac{1}{r_{12}} \\phi_\\lambda(\\mathbf{r}_2) \\phi_\\sigma(\\mathbf{r}_2) \\, d\\mathbf{r}_1 d\\mathbf{r}_2.\\]\nWe can again use the einsum-function here to construct these matrices.\n\nJmat = np.einsum('kl, ijkl-&gt;ij', Dmat, Vee, optimize=True) # Compute the Coulomb integrals\nKmat = np.einsum('kl, ikjl-&gt;ij', Dmat, Vee, optimize=True) # Compute the exchange integrals\n\nNow we have everything we need to run the SCF cycle! However, we still need to have a look at how we exactly compute the total electronic energy of the system: as it turns out, the total electronic energy of the Hartree-Fock system is not the sum of the eigenvalues of the occupied MOs."
  },
  {
    "objectID": "blog/blog_buildhf_part4.html",
    "href": "blog/blog_buildhf_part4.html",
    "title": "Let’s build Hartree-Fock part 4: self-consistent field",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nThis tutorial covers topics from sections 3.4 and 3.5 from the Modern Quantum Chemistry by Szabo and Ostlund.\n\nThe self consistent field\nNow we have everything we need to run the SCF cycle! We’ll combine here everything we have built so far to recap.\nWe also need to define some parameters for our SCF loop: a threshold for convergence and the maximum number of iterations, so that our loop has a definite end point. We’ll print out the energy and the matrix norm of the difference between the new and old density matrices.\n\nfrom pyscf import gto, scf, mp\nfrom ase.units import Bohr\nimport numpy as np\n\n# Define the system in PySCF and get all the molecular integrals\nm = gto.Mole()\nm.build(atom=f\"H 0 0 0; H {1.4*Bohr} 0 0\", basis=\"3-21g\", spin=0, charge=0)\nTmat = m.intor(\"int1e_kin\") # Integral matrix for kinetic energy\nSmat = m.intor(\"int1e_ovlp\") # Integral matrix for overlap integrals\nVmat = m.intor(\"int1e_nuc\") # Integral matrix for electron-nucleus integrals \nVee = m.intor(\"int2e\") # Integral matrix for electron-electron integrals\nnelec = m.nelectron # Number of electrons\n\n\n# Orthogonalization matrix\ns, U = np.linalg.eigh(Smat)\nX = U / s**0.5\n\n# Core Hamiltonian initial guess, eqn 3.153\nHcore = Tmat + Vmat\nHcore_ = np.dot(X.T, Hcore.dot(X)) # Transform from AO to MO basis\nmo_eigs, mo_vecs = np.linalg.eigh(Hcore_) # Diagonalize the Hcore in MO basis\n# Construct the density matrix from the initial guess\nCvec = np.dot(X, mo_vecs) # Transform the MO coefficients from MO to AO basis\nDmat = np.zeros((Smat.shape)) # Initialize the density matrix\nfor a in range(nelec//2): # Compute the density matrix in the AO basis. Since we use restricted HF, all spins are paired and only nelec/2 orbitals are considered\n    Dmat += np.einsum('i,j-&gt;ij', Cvec[:, a], Cvec[:, a].T)\n\nWhat exactly is the electronic energy, \\(E_0\\), of the Hartree-Fock system? One might think, that since we have \\(N\\) electrons, the total energy would be the sum of the eigenenergies for each electron, \\(\\sum\\limits_a^N \\epsilon_a\\). However, this is not correct: if we just add all the eigenvalues together, we end up counting the electron-electron interactions between the electrons twice (see Szabo and Ostlund, Modern quantum chemistry eqns: 3.81-3.82):\n\\[\\sum\\limits_a^N \\epsilon_a = \\sum\\limits_a^N \\left&lt;a\\left|h\\right|a\\right&gt; + \\sum\\limits_a^N \\sum\\limits_b^N \\left[ \\left&lt;aa\\left|\\right.bb\\right&gt; - \\left&lt;ab\\left|\\right.ba\\right&gt;\\right] \\]\n\\[E_0 = \\sum\\limits_a^N \\left&lt;a\\left|h\\right|a\\right&gt; + \\frac{1}{2} \\sum\\limits_a^N  \\sum\\limits_b^N \\left[ \\left&lt;aa\\left|\\right.bb\\right&gt; - \\left&lt;ab\\left|\\right.ba\\right&gt;\\right] \\]\nAbove, \\(h\\) is the core Hamiltonian and \\(a\\) and \\(b\\) correspond to the occupied orbitals.\nLet’s have a look at the molecular orbitals and their energies. We will be using the Aufbau-principle to populate the molecular orbitals: populate the orbitals from the lowest orbital upwards. Since we are using the Numpy ‘eigh’, the eigenvalues are already ordered from the lowest eigenvalue to the highest.\n\nmaxiter = 150\nconv_tol = 1e-8\niterstep = 0\nconv = 1e0\n\nprint(f\"{'Iteration':&gt;10} {'Total Energy':&gt;15} {'Dmat Difference':&gt;18}\")\nwhile iterstep &lt; maxiter:\n    # Update the two-electron part\n    Jmat = np.einsum('kl, ijkl-&gt;ij', Dmat, Vee, optimize=True) # Compute the Coulomb integrals\n    Kmat = np.einsum('kl, ikjl-&gt;ij', Dmat, Vee, optimize=True) # Compute the exchange integrals\n\n    Fmat = Hcore + 2*Jmat - Kmat # Construct the full Fock matrix, eqn 3.154\n    e_elec = np.sum(Dmat * (Hcore + Fmat)) # Compute the electronic energy, eqn 3.184\n    Dmat_old = Dmat.copy() # Store old density matrix\n\n    Fmat_ = np.dot(X.T, Fmat.dot(X)) # Transform the Fockian from the AO to the orthonormal MO basis\n    mo_eigs, mo_vecs = np.linalg.eigh(Fmat_) # Solve the Fockian\n    Cvec = np.dot(X, mo_vecs) # Transform the MO coefficients from MO basis to AO basis\n\n    # Construct a new density matrix\n    Dmat *= 0\n    for a in range(nelec//2):\n        Dmat += np.einsum('i,j-&gt;ij', Cvec[:, a], Cvec[:, a].T)\n       \n    conv = np.linalg.norm(Dmat-Dmat_old) # Evaluate difference in the density matrices\n    iterstep += 1\n    print(f\"{iterstep:&gt;10} {e_elec:&gt;15.10f} {conv:&gt;18.3e}\") # Print out energy\n    if conv &lt; conv_tol: # Check convergence. If converged, end cycle\n        break\n\n Iteration    Total Energy    Dmat Difference\n         1   -1.7873257912          1.533e-01\n         2   -1.8361123242          2.198e-02\n         3   -1.8371958862          3.175e-03\n         4   -1.8372185962          4.576e-04\n         5   -1.8372190684          6.595e-05\n         6   -1.8372190782          9.502e-06\n         7   -1.8372190784          1.369e-06\n         8   -1.8372190784          1.973e-07\n         9   -1.8372190784          2.843e-08\n        10   -1.8372190784          4.096e-09\n\n\n\n\nQuestions\n\nTry changing the molecule from H2 to H2O and see how the SCF cycle performs. What if you increase the size of the basis set, say, to aug-cc-pvdz? What do you observe?\n\n\nm = gto.Mole()\nm.build(atom=f\"O   0.00000000000000     -0.00000000000822      0.06643866886100;  \\\n                H   0.00000000000000      0.75804982860716     -0.52326933443332; \\\n                H   0.00000000000000     -0.75804982859894     -0.52326933442768\", basis=\"aug-cc-pvdz\", spin=0, charge=0)\nTmat = m.intor(\"int1e_kin\")\nSmat = m.intor(\"int1e_ovlp\")\nVmat = m.intor(\"int1e_nuc\")\nVee = m.intor(\"int2e\")\nnelec = m.nelectron\n\n\nInstead of using 100% new density at each step, try mixing the old and new density matrices: Dmat = (Dmat + Dmat_old) / 2. This is called damping. How does it affect the SCF convergence for water molecule with a larger basis set?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Quantum of Science\n\n\n\n\n\nAbout this site"
  }
]